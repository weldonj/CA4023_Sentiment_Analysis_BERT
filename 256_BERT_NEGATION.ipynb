{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT with Negation Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was something fairly silly that I wanted to just take a quick look at. In theory, BERT should be able to handle this negation by itself as it can understand the context of negation, having seen many many examples of it in it's training. But with that said, I still felt it would be useful to take a look at how it performed.\n",
    "\n",
    "For this I used the documents that I created in the previous assignment with NOT_ added to any tokens that follow an 'n't' and occurr before the next piece of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_256_neg_files = ['256_BERT_NEG/1_pred_negation.txt']\n",
    "\n",
    "c_names = ['gold','pred','correct','text']\n",
    "\n",
    "df1_neg = pd.DataFrame(columns=c_names)\n",
    "\n",
    "dataframes_256_neg = [df1_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs(files, df_list):\n",
    "    j = 0\n",
    "    for dataframe in df_list:\n",
    "\n",
    "        #dataframe = pd.DataFrame(columns=['index','gold','pred','correct','text'])\n",
    "        processed_lines = []\n",
    "\n",
    "        with open(files[j], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            count = 0\n",
    "            for line in lines[1:]:\n",
    "                tokens = line.split()\n",
    "                line_length = len(tokens)\n",
    "                temp_line = ''\n",
    "\n",
    "                for i in range(4, (line_length)):\n",
    "                    temp_line = temp_line + tokens[i] + ' '\n",
    "\n",
    "                processed_line = [tokens[1],tokens[2],tokens[3], temp_line]\n",
    "                processed_lines.append(processed_line)\n",
    "                dataframe.loc[count] = processed_line\n",
    "                count+=1\n",
    "        j+=1\n",
    "    return(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_256_neg = create_dfs(bert_256_neg_files, dataframes_256_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(dataframe):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    corrects = 0\n",
    "    errors = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        if dataframe.iat[i,2] == 'yes':\n",
    "            corrects += 1\n",
    "        else:\n",
    "            errors.append(i)\n",
    "        if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "            true_pos += 1\n",
    "        elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "            false_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "            true_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "            false_pos += 1\n",
    "    \n",
    "    accuracy = corrects/len(dataframe)\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    f1_score = 2*((precision*recall)/(precision + recall))\n",
    "    return(accuracy,precision,recall,f1_score,errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averages(df_list):\n",
    "    accuracies = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    errors_list = []\n",
    "    for dataframe in df_list:    \n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "        corrects = 0\n",
    "        errors = []\n",
    "        for i in range(0,len(dataframe)):\n",
    "            if dataframe.iat[i,2] == 'yes':\n",
    "                corrects += 1\n",
    "            else:\n",
    "                errors.append(i)\n",
    "            if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "                true_pos += 1\n",
    "            elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "                false_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "                true_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "                false_pos += 1\n",
    "\n",
    "        accuracy = corrects/len(dataframe)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "        precs.append(precision)\n",
    "\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "        recs.append(recall)\n",
    "        \n",
    "        f1_score = 2*((precision*recall)/(precision + recall))\n",
    "        f1s.append(f1_score)\n",
    "        \n",
    "        errors_list.append(errors)\n",
    "        \n",
    "    return(sum(accuracies)/len(df_list),sum(precs)/len(df_list),sum(recs)/len(df_list),sum(f1s)/len(df_list), errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_averages_get_errors(dataframes, errorlist = False):\n",
    "    acc,prec,rec,f1,errors = get_averages(dataframes)\n",
    "    if errorlist == True:\n",
    "        return(errors)\n",
    "    else:\n",
    "        for i, dataframe in enumerate(dataframes):\n",
    "            scores = get_f1(dataframe)\n",
    "            print(f'Cross validation {i+1}')\n",
    "            print(f'The accuracy is {scores[0]*100:.2f}%')\n",
    "            print(f'The precision is {scores[1]*100:.2f}%')\n",
    "            print(f'The recall is {scores[2]*100:.2f}%')\n",
    "            print(f'The F1 score is {scores[3]*100:.2f}%')\n",
    "            print(f'The model got the following rows wrong {scores[4]}\\n')\n",
    "\n",
    "        print(f'The average accuracy is {acc*100:.2f}%')\n",
    "        print(f'The average precision is {prec*100:.2f}%')\n",
    "        print(f'The average recall is {rec*100:.2f}%')\n",
    "        print(f'The average F1 score is {f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 1\n",
      "The accuracy is 86.50%\n",
      "The precision is 90.11%\n",
      "The recall is 82.00%\n",
      "The F1 score is 85.86%\n",
      "The model got the following rows wrong [1, 4, 10, 19, 22, 24, 25, 40, 44, 46, 50, 72, 76, 82, 89, 91, 95, 97, 108, 118, 129, 134, 135, 157, 173, 177, 198]\n",
      "\n",
      "The average accuracy is 86.50%\n",
      "The average precision is 90.11%\n",
      "The average recall is 82.00%\n",
      "The average F1 score is 85.86%\n"
     ]
    }
   ],
   "source": [
    "print_averages_get_errors(dataframes_256_neg, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only performed this experiment on the first CV fold because as I mentioned in the first cell in this notebook, it ultimately does not seem like a very good idea. The results here show that the average accuracy has suffered quite significantly vs the original document set with no negation handling. I would expect this to be the case across the other 9 CV folds. Interestingly, it still outperforms the Baseline NB model from Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
