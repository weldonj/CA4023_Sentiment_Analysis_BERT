{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Implementation\n",
    "\n",
    "This is essentially the same as the previous bigram implementation but this time we append the tokens as groups of three strings instead of two strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = PL04DataLoaderFromTGZ('data.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== pos ==\n",
      "doc sentences start of first sentence\n",
      "  0      25   films adapted from|adapted from comic|from comic books|comic books have\n",
      "  1      39   every now and|now and then|and then a|then a movie|a movie comes\n",
      "  2      19   you've got mail|got mail works|mail works alot|works alot better\n",
      "  3      42   \" jaws \"|jaws \" is|\" is a|is a rare|a rare film|rare film that\n",
      "  4      25   moviemaking is a|is a lot|a lot like|lot like being|like being the\n",
      "== neg ==\n",
      "doc sentences start of first sentence\n",
      "  0      35   plot : two|: two teen|two teen couples|teen couples go|couples go to\n",
      "  1      13   the happy bastard's|happy bastard's quick|bastard's quick movie\n",
      "  2      23   it is movies|is movies like|movies like these|like these that\n",
      "  3      19   \" quest for|quest for camelot|for camelot \"|camelot \" is|\" is warner\n",
      "  4      37   synopsis : a|: a mentally|a mentally unstable|mentally unstable man\n"
     ]
    }
   ],
   "source": [
    "# test \"get_documents()\"\n",
    "\n",
    "def get_document_preview(document, max_length = 72):\n",
    "    s = []\n",
    "    count = 0\n",
    "    reached_limit = False\n",
    "    for sentence in document:\n",
    "        i = 0\n",
    "        while (i < len(sentence) - 2):\n",
    "            token = sentence[i] + ' ' + sentence[i+1] + ' ' + sentence[i+2]\n",
    "            if count + len(token) + len(s) > max_length:\n",
    "                reached_limit = True\n",
    "                break\n",
    "\n",
    "            s.append(token)\n",
    "            count += len(token)\n",
    "            i+=1\n",
    "        if reached_limit:\n",
    "            break\n",
    "    return '|'.join(s)\n",
    "    \n",
    "for label in 'pos neg'.split():\n",
    "    print(f'== {label} ==')\n",
    "    print('doc sentences start of first sentence')\n",
    "    for index, document in enumerate(data_loader.get_documents(\n",
    "        label = label\n",
    "    )):\n",
    "        print('%3d %7d   %s' %(\n",
    "            index, len(document), get_document_preview(document)\n",
    "        ))\n",
    "        if index == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr-size te-size (number of documents)\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n"
     ]
    }
   ],
   "source": [
    "# test \"get_xval_splits()\"\n",
    "\n",
    "splits = data_loader.get_xval_splits()\n",
    "\n",
    "print('tr-size te-size (number of documents)')\n",
    "for xval_tr_data, xval_te_data in splits:\n",
    "    print('%7d %7d' %(len(xval_tr_data), len(xval_te_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorInterface:\n",
    "\n",
    "    def train(self, data_with_labels):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, data):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorWithVocabulary(PolarityPredictorInterface):\n",
    "    \n",
    "    def train(self, data_with_labels):\n",
    "        self.reset_vocab()\n",
    "        self.add_to_vocab_from_data(data_with_labels)\n",
    "        self.finalise_vocab()\n",
    "        tr_features = self.extract_features(\n",
    "            data_with_labels\n",
    "        )\n",
    "        tr_targets = self.get_targets(data_with_labels)\n",
    "        self.train_model_on_features(tr_features, tr_targets)\n",
    "        \n",
    "    def reset_vocab(self):\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def add_to_vocab_from_data(self, data):\n",
    "        for document, label in data:\n",
    "            for sentence in document:\n",
    "                i = 0\n",
    "                while (i < len(sentence) - 2):\n",
    "                    token = sentence[i] + ' ' + sentence[i+1] + ' ' + sentence[i+2]\n",
    "                    self.vocab.add(token)\n",
    "                    i+=1\n",
    "\n",
    "    def finalise_vocab(self):\n",
    "        self.vocab = list(self.vocab)\n",
    "        # create reverse map for fast token lookup\n",
    "        self.token2index = {}\n",
    "        for index, token in enumerate(self.vocab):\n",
    "            self.token2index[token] = index\n",
    "        \n",
    "    def extract_features(self, data):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_targets(self, data, label2index = None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class PolarityPredictorWithBagOfWords_01(PolarityPredictorWithVocabulary):\n",
    "    \n",
    "    def __init__(self, clip_counts = True):\n",
    "        self.clip_counts = clip_counts\n",
    "        \n",
    "    def extract_features(self, data):\n",
    "        # create numpy array of required size\n",
    "        columns = len(self.vocab)\n",
    "        rows = len(data)\n",
    "        features = numpy.zeros((rows, columns), dtype=numpy.int32)        \n",
    "        # populate feature matrix\n",
    "        for row, item in enumerate(data):\n",
    "            document, _ = item\n",
    "            for sentence in document:\n",
    "\n",
    "                i = 0\n",
    "                while (i < len(sentence)-2):\n",
    "                    token = sentence[i] + ' ' + sentence[i+1] + ' ' + sentence[i+2]\n",
    "                    i+=1\n",
    "\n",
    "                    try:\n",
    "                        index = self.token2index[token]\n",
    "                    except KeyError:\n",
    "                        # token not in vocab\n",
    "                        # --> skip this token\n",
    "                        # --> continue with next token\n",
    "                        continue\n",
    "                    if self.clip_counts:\n",
    "                        features[row, index] = 1\n",
    "                    else:\n",
    "                        features[row, index] += 1\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorWithBagOfWords(PolarityPredictorWithBagOfWords_01):\n",
    " \n",
    "    def get_targets(self, data):\n",
    "        ''' create column vector with target labels\n",
    "        '''\n",
    "        # prepare target vector\n",
    "        targets = numpy.zeros(len(data), dtype=numpy.int8)\n",
    "        index = 0\n",
    "        for _, label in data:\n",
    "            if label == 'pos':\n",
    "                targets[index] = 1\n",
    "            index += 1\n",
    "        return targets\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "class PolarityPredictorBowNB(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train NB\n",
    "        self.model = MultinomialNB()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first functionality test\n",
    "\n",
    "model = PolarityPredictorBowNB()\n",
    "model.train(splits[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 pos pos films adapted from|adapted from comic|from comic books|comic books have\n",
      "   1 pos pos every now and|now and then|and then a|then a movie|a movie comes\n",
      "   2 pos pos you've got mail|got mail works|mail works alot|works alot better\n",
      "   3 pos pos \" jaws \"|jaws \" is|\" is a|is a rare|a rare film|rare film that\n",
      "   4 pos neg moviemaking is a|is a lot|a lot like|lot like being|like being the\n",
      "   5 pos pos on june 30|june 30 ,|30 , 1960|, 1960 ,|1960 , a|, a self-taught\n",
      "   6 pos pos apparently , director|, director tony|director tony kaye|tony kaye had\n",
      "   7 pos pos one of my|of my colleagues|my colleagues was|colleagues was surprised\n",
      "   8 pos pos after bloody clashes|bloody clashes and|clashes and independence\n",
      "   9 pos pos the american action|american action film|action film has|film has been\n",
      "  10 pos pos after watching \"|watching \" rat|\" rat race|rat race \"|race \" last\n",
      "  11 pos pos i've noticed something|noticed something lately|something lately that\n"
     ]
    }
   ],
   "source": [
    "def print_first_predictions(model, te_data, n = 12):\n",
    "    predictions = model.predict(te_data)\n",
    "    for i in range(n):\n",
    "        document, label = te_data[i]\n",
    "        prediction = predictions[i]\n",
    "        print('%4d %s %s %s' %(\n",
    "            i, label, prediction,\n",
    "            get_document_preview(document),\n",
    "        ))\n",
    "    \n",
    "print_first_predictions(model, splits[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n",
      "[[77 23]\n",
      " [13 87]]\n"
     ]
    }
   ],
   "source": [
    "labels, accuracy, confusion_matrix = model.predict(\n",
    "    splits[0][1], get_accuracy = True, get_confusion_matrix = True\n",
    ")\n",
    "\n",
    "print(accuracy)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.82\n",
      "Precision --> 0.77\n",
      "Recall --> 0.8555555555555555\n",
      "F1 --> 0.8105263157894737\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.815\n",
      "Precision --> 0.78\n",
      "Recall --> 0.8387096774193549\n",
      "F1 --> 0.8082901554404146\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.83\n",
      "Recall --> 0.8556701030927835\n",
      "F1 --> 0.8426395939086294\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.8\n",
      "Recall --> 0.8888888888888888\n",
      "F1 --> 0.8421052631578948\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.855\n",
      "Precision --> 0.83\n",
      "Recall --> 0.8736842105263158\n",
      "F1 --> 0.8512820512820513\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.825\n",
      "Precision --> 0.77\n",
      "Recall --> 0.8651685393258427\n",
      "F1 --> 0.8148148148148148\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.865\n",
      "Precision --> 0.81\n",
      "Recall --> 0.9101123595505618\n",
      "F1 --> 0.8571428571428572\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.83\n",
      "Precision --> 0.75\n",
      "Recall --> 0.8928571428571429\n",
      "F1 --> 0.8152173913043479\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8484848484848485\n",
      "F1 --> 0.8442211055276383\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.84\n",
      "Precision --> 0.8\n",
      "Recall --> 0.8695652173913043\n",
      "F1 --> 0.8333333333333333\n",
      "\n",
      "(0.8319572881701454, 0.01684042131059596, 0.8082901554404146, 0.8571428571428572)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, splits, verbose = False):\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    fold = 0\n",
    "    for tr_data, te_data in splits:\n",
    "        if verbose:\n",
    "            print('Evaluating fold %d of %d' %(fold+1, len(splits)))\n",
    "            fold += 1\n",
    "        model.train(tr_data)\n",
    "        _, accuracy, confusion_matrix = model.predict(te_data, get_accuracy = True, get_confusion_matrix = True)\n",
    "        \n",
    "        tp, fp, fn, tn = confusion_matrix[0][0], confusion_matrix[0][1], confusion_matrix[1][0], confusion_matrix[1][1]\n",
    "        prec = tp/(tp + fp)\n",
    "        rec = tp/(tp + fn)\n",
    "        f1 = (2*prec*rec)/(prec+rec)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "        if verbose:\n",
    "            print('Accuracy -->', accuracy)\n",
    "            print('Precision -->', prec)\n",
    "            print('Recall -->', rec)\n",
    "            print('F1 -->', f1)\n",
    "            print()\n",
    "    n = float(len(accuracies))\n",
    "    avg = sum(f1s) / n\n",
    "    mse = sum([(x-avg)**2 for x in accuracies]) / n\n",
    "    return (avg, mse**0.5, min(f1s),\n",
    "            max(f1s))\n",
    "\n",
    "# this takes about 3 minutes\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "class PolarityPredictorBowLR(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolarityPredictorBowLR()\n",
    "model.train(splits[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 pos neg films adapted from|adapted from comic|from comic books|comic books have\n",
      "   1 pos pos every now and|now and then|and then a|then a movie|a movie comes\n",
      "   2 pos pos you've got mail|got mail works|mail works alot|works alot better\n",
      "   3 pos pos \" jaws \"|jaws \" is|\" is a|is a rare|a rare film|rare film that\n",
      "   4 pos neg moviemaking is a|is a lot|a lot like|lot like being|like being the\n",
      "   5 pos pos on june 30|june 30 ,|30 , 1960|, 1960 ,|1960 , a|, a self-taught\n",
      "   6 pos pos apparently , director|, director tony|director tony kaye|tony kaye had\n",
      "   7 pos pos one of my|of my colleagues|my colleagues was|colleagues was surprised\n",
      "   8 pos neg after bloody clashes|bloody clashes and|clashes and independence\n",
      "   9 pos pos the american action|american action film|action film has|film has been\n",
      "  10 pos neg after watching \"|watching \" rat|\" rat race|rat race \"|race \" last\n",
      "  11 pos neg i've noticed something|noticed something lately|something lately that\n"
     ]
    }
   ],
   "source": [
    "print_first_predictions(model, splits[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725\n",
      "[[84 16]\n",
      " [39 61]]\n"
     ]
    }
   ],
   "source": [
    "labels, accuracy, confusion_matrix = model.predict(\n",
    "    splits[0][1], get_accuracy = True, get_confusion_matrix = True\n",
    ")\n",
    "\n",
    "print(accuracy)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.725\n",
      "Precision --> 0.84\n",
      "Recall --> 0.6829268292682927\n",
      "F1 --> 0.7533632286995516\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.815\n",
      "Precision --> 0.91\n",
      "Recall --> 0.7647058823529411\n",
      "F1 --> 0.8310502283105021\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.79\n",
      "Precision --> 0.92\n",
      "Recall --> 0.7301587301587301\n",
      "F1 --> 0.8141592920353983\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.785\n",
      "Precision --> 0.89\n",
      "Recall --> 0.7355371900826446\n",
      "F1 --> 0.8054298642533936\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.76\n",
      "Precision --> 0.9\n",
      "Recall --> 0.703125\n",
      "F1 --> 0.7894736842105263\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.76\n",
      "Precision --> 0.78\n",
      "Recall --> 0.75\n",
      "F1 --> 0.7647058823529411\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.745\n",
      "Precision --> 0.81\n",
      "Recall --> 0.7168141592920354\n",
      "F1 --> 0.7605633802816901\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.78\n",
      "Precision --> 0.86\n",
      "Recall --> 0.7413793103448276\n",
      "F1 --> 0.7962962962962963\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.81\n",
      "Precision --> 0.91\n",
      "Recall --> 0.7583333333333333\n",
      "F1 --> 0.8272727272727273\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.775\n",
      "Precision --> 0.88\n",
      "Recall --> 0.7272727272727273\n",
      "F1 --> 0.7963800904977375\n",
      "\n",
      "(0.7938694674210763, 0.03282417201051897, 0.7533632286995516, 0.8310502283105021)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having ran the Naive Bayes and Logistic Regression algorithms with Trigrams and seeing that both actually perform worse than their Bigram counterparts with much longer run times, I decided there was no point proceeding with Decision Tree and Support Vector Machine.\n",
    "\n",
    "This seemed likely to not produce any better performance and would have a huge time cost.\n",
    "\n",
    "The next step was to see what the combination of negation handling alongside bigrams might achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
