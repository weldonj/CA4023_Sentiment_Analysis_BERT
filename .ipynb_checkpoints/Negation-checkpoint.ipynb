{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation Handling\n",
    "\n",
    "I wanted to add the Negation Handling to original dataset. The basic idea is that if a word in any of our sentences ends in \"n't\" then any words that follow it should have 'NOT_' added as a prefix. This should continue until the next piece of punctuation appears within the sentence. Once this happens, we continue as normal until the next instance of \"n't\" is found.\n",
    "\n",
    "The following four cells start by reading in all of the files contained in our positive folder and our negative folder. \n",
    "\n",
    "The function \"handle_negation\" iterates through the full list of files, finds any incidents of \"n't\", adds the \"NOT_\" prefix to words that follow up to the next punctuation.\n",
    "\n",
    "It then saves the new document with this added negation handling back to a seperate folder.\n",
    "\n",
    "Here is an example of what a sentence looks like before and after this is implemented:\n",
    "\n",
    "Without negation handling - \"i don't think anyone needs to be briefed on jack the ripper,\"\n",
    "\n",
    "With negation handling - \"i don't NOT_think NOT_anyone NOT_needs NOT_to NOT_be NOT_briefed NOT_on NOT_jack NOT_the NOT_ripper,\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "pos_path = \"./data/txt_sentoken/pos/\"\n",
    "pos_path_out = \"./data/txt_sentoken_negation/pos_negation/\"\n",
    "neg_path = \"./data/txt_sentoken/neg/\"\n",
    "neg_path_out = \"./data/txt_sentoken_negation/neg_negation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_negation(in_path, out_path):\n",
    "    file_list = os.listdir(in_path)\n",
    "    for file in file_list:\n",
    "        new_file = file\n",
    "        new_file_sentences = []\n",
    "        with open(in_path + file, 'r') as f, open(out_path + new_file, 'w+') as f_out:\n",
    "            for line in f.readlines():\n",
    "                new_line = ''\n",
    "                tokens = line.split()\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "\n",
    "                    if tokens[i][-3:] != \"n't\":\n",
    "                        new_line = new_line + tokens[i] + ' '\n",
    "                        i+=1\n",
    "                    \n",
    "                    else:\n",
    "                        new_line = new_line + tokens[i] + ' '\n",
    "                        try:\n",
    "                            while tokens[i+1] not in string.punctuation:\n",
    "                                new_line = new_line + 'NOT_' + tokens[i+1] + ' '\n",
    "                                i+=1\n",
    "                        except:\n",
    "                            print(\"end of sentence\")\n",
    "                        i+=1\n",
    "                new_file_sentences.append(new_line + '\\n')\n",
    "                \n",
    "            f_out.writelines(new_file_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/txt_sentoken_negation/pos_negation/cv000_29590.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c3d283c6af7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhandle_negation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_path_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-eff5811f5d0e>\u001b[0m in \u001b[0;36mhandle_negation\u001b[1;34m(in_path, out_path)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnew_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mnew_file_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf_out\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                 \u001b[0mnew_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/txt_sentoken_negation/pos_negation/cv000_29590.txt'"
     ]
    }
   ],
   "source": [
    "handle_negation(pos_path, pos_path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_negation(neg_path, neg_path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final outcome is two folders that match our original ones in structure, containing the same documents as the original with negation handling.\n",
    "\n",
    "This means we can now continue from this point like we did with baseline and experiment in the same way with various algorithms and compare them to each other and also to the baseline versions that didn't have negation handling.\n",
    "\n",
    "Like the previous page, it is best to skip down to the \"Naive Bayes\" section at this point to start looking at the experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "class PL04DataLoader_Part_1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_labelled_dataset(self, fold = 0):\n",
    "        ''' Compile a fold of the data set\n",
    "        '''\n",
    "        dataset = []\n",
    "        for label in ('pos_negation', 'neg_negation'):\n",
    "            for document in self.get_documents(\n",
    "                fold = fold,\n",
    "                label = label,\n",
    "            ):\n",
    "                dataset.append((document, label))\n",
    "        return dataset\n",
    "    \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        ''' Enumerate the raw contents of all data set files.\n",
    "            Args:\n",
    "                data_dir: relative or absolute path to the data set folder\n",
    "                fold: which fold to load (0 to n_folds-1)\n",
    "                label: 'pos' or 'neg' to\n",
    "                    select data with positive or negative sentiment\n",
    "                    polarity\n",
    "            Return:\n",
    "                List of tokenised documents, each a list of sentences\n",
    "                that in turn are lists of tokens\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class PL04DataLoader(PL04DataLoader_Part_1):\n",
    "    \n",
    "    def get_xval_splits(self):\n",
    "        ''' Split data with labels for cross-validation\n",
    "            returns a list of k pairs (training_data, test_data)\n",
    "            for k cross-validation\n",
    "        '''\n",
    "        # load the folds\n",
    "        folds = []\n",
    "        for i in range(10):\n",
    "            folds.append(self.get_labelled_dataset(\n",
    "                fold = i\n",
    "            ))\n",
    "        # create training-test splits\n",
    "        retval = []\n",
    "        for i in range(10):\n",
    "            test_data = folds[i]\n",
    "            training_data = []\n",
    "            for j in range(9):\n",
    "                ij1 = (i+j+1) % 10\n",
    "                assert ij1 != i\n",
    "                training_data = training_data + folds[ij1]\n",
    "            retval.append((training_data, test_data))\n",
    "        return retval\n",
    "    \n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "class PL04DataLoaderFromStream(PL04DataLoader):\n",
    "        \n",
    "    def __init__(self, tgz_stream, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        with tarfile.open(\n",
    "            mode = 'r|gz',\n",
    "            fileobj = tgz_stream\n",
    "        ) as tar_archive:\n",
    "            for tar_member in tar_archive:\n",
    "                if counter == 2000:\n",
    "                    break\n",
    "                path_components = tar_member.name.split('/')\n",
    "                filename = path_components[-1]\n",
    "                if filename.startswith('cv') \\\n",
    "                and filename.endswith('.txt') \\\n",
    "                and '_' in filename:\n",
    "                    label = path_components[-2]\n",
    "                    fold = int(filename[2])\n",
    "                    key = (fold, label)\n",
    "                    if key not in self.data:\n",
    "                        self.data[key] = []\n",
    "                    f = tar_archive.extractfile(tar_member)\n",
    "                    document = [\n",
    "                        line.decode('utf-8').split()\n",
    "                        for line in f.readlines()\n",
    "                    ]\n",
    "                    self.data[key].append(document)\n",
    "                    counter += 1\n",
    "            \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        return self.data[(fold, label)]\n",
    "    \n",
    "\n",
    "\n",
    "class PL04DataLoaderFromFolder(PL04DataLoader):\n",
    "    \n",
    "    def __init__(self, data_dir, **kwargs):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def get_documents(self, fold = 0, label = 'pos_negation'):\n",
    "        # read folder contents\n",
    "        path = os.path.join(self.data_dir, label)\n",
    "        dir_entries = os.listdir(path)\n",
    "        # must process entries in numeric order to\n",
    "        # replicate order of original experiments\n",
    "        dir_entries.sort()\n",
    "        # check each entry and add to data if matching\n",
    "        # selection criteria\n",
    "        for filename in dir_entries:\n",
    "            if filename.startswith('cv') \\\n",
    "            and filename.endswith('.txt'):\n",
    "                if fold == int(filename[2]):\n",
    "                    # correct fold\n",
    "                    f = open(os.path.join(path, filename), 'rt')\n",
    "                    # \"yield\" tells Python to return an iterator\n",
    "                    # object that produces the yields of this\n",
    "                    # function as elements without creating a\n",
    "                    # full list of all elements\n",
    "                    yield [line.split() for line in f.readlines()]\n",
    "                    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_entries = os.listdir()\n",
    "dir_entries.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = PL04DataLoaderFromFolder(\"./data/txt_sentoken_negation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== pos_negation ==\n",
      "doc sentences start of first sentence\n",
      "  0      25   films|adapted|from|comic|books|have|had|plenty|of|success|,|whether\n",
      "  1      39   every|now|and|then|a|movie|comes|along|from|a|suspect|studio|,|with\n",
      "  2      19   you've|got|mail|works|alot|better|than|it|deserves|to|.|in|order|to|make\n",
      "  3      42   \"|jaws|\"|is|a|rare|film|that|grabs|your|attention|before|it|shows|you|a\n",
      "  4      25   moviemaking|is|a|lot|like|being|the|general|manager|of|an|nfl|team|in\n",
      "== neg_negation ==\n",
      "doc sentences start of first sentence\n",
      "  0      35   plot|:|two|teen|couples|go|to|a|church|party|,|drink|and|then|drive|.\n",
      "  1      13   the|happy|bastard's|quick|movie|review|damn|that|y2k|bug|.|it's|got|a\n",
      "  2      23   it|is|movies|like|these|that|make|a|jaded|movie|viewer|thankful|for|the\n",
      "  3      19   \"|quest|for|camelot|\"|is|warner|bros|.|'|first|feature-length|,\n",
      "  4      37   synopsis|:|a|mentally|unstable|man|undergoing|psychotherapy|saves|a|boy\n"
     ]
    }
   ],
   "source": [
    "# test \"get_documents()\"\n",
    "\n",
    "def get_document_preview(document, max_length = 72):\n",
    "    s = []\n",
    "    count = 0\n",
    "    reached_limit = False\n",
    "    for sentence in document:\n",
    "        for token in sentence:\n",
    "            if count + len(token) + len(s) > max_length:\n",
    "                reached_limit = True\n",
    "                break\n",
    "            s.append(token)\n",
    "            count += len(token)\n",
    "        if reached_limit:\n",
    "            break\n",
    "    return '|'.join(s)\n",
    "    \n",
    "for label in 'pos_negation neg_negation'.split():\n",
    "    print(f'== {label} ==')\n",
    "    print('doc sentences start of first sentence')\n",
    "    for index, document in enumerate(data_loader.get_documents(\n",
    "        label = label\n",
    "    )):\n",
    "        print('%3d %7d   %s' %(\n",
    "            index, len(document), get_document_preview(document)\n",
    "        ))\n",
    "        if index == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr-size te-size (number of documents)\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n"
     ]
    }
   ],
   "source": [
    "# test \"get_xval_splits()\"\n",
    "\n",
    "splits = data_loader.get_xval_splits()\n",
    "\n",
    "print('tr-size te-size (number of documents)')\n",
    "for xval_tr_data, xval_te_data in splits:\n",
    "    print('%7d %7d' %(len(xval_tr_data), len(xval_te_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorInterface:\n",
    "\n",
    "    def train(self, data_with_labels):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, data):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorWithVocabulary(PolarityPredictorInterface):\n",
    "    \n",
    "    def train(self, data_with_labels):\n",
    "        self.reset_vocab()\n",
    "        self.add_to_vocab_from_data(data_with_labels)\n",
    "        self.finalise_vocab()\n",
    "        tr_features = self.extract_features(\n",
    "            data_with_labels\n",
    "        )\n",
    "        tr_targets = self.get_targets(data_with_labels)\n",
    "        self.train_model_on_features(tr_features, tr_targets)\n",
    "        \n",
    "    def reset_vocab(self):\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def add_to_vocab_from_data(self, data):\n",
    "        for document, label in data:\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    self.vocab.add(token)\n",
    "\n",
    "    def finalise_vocab(self):\n",
    "        self.vocab = list(self.vocab)\n",
    "        # create reverse map for fast token lookup\n",
    "        self.token2index = {}\n",
    "        for index, token in enumerate(self.vocab):\n",
    "            self.token2index[token] = index\n",
    "        \n",
    "    def extract_features(self, data):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_targets(self, data, label2index = None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class PolarityPredictorWithBagOfWords_01(PolarityPredictorWithVocabulary):\n",
    "    \n",
    "    def __init__(self, clip_counts = True):\n",
    "        self.clip_counts = clip_counts\n",
    "        \n",
    "    def extract_features(self, data):\n",
    "        # create numpy array of required size\n",
    "        columns = len(self.vocab)\n",
    "        rows = len(data)\n",
    "        features = numpy.zeros((rows, columns), dtype=numpy.int32)        \n",
    "        # populate feature matrix\n",
    "        for row, item in enumerate(data):\n",
    "            document, _ = item\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    try:\n",
    "                        index = self.token2index[token]\n",
    "                    except KeyError:\n",
    "                        # token not in vocab\n",
    "                        # --> skip this token\n",
    "                        # --> continue with next token\n",
    "                        continue\n",
    "                    if self.clip_counts:\n",
    "                        features[row, index] = 1\n",
    "                    else:\n",
    "                        features[row, index] += 1\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorWithBagOfWords(PolarityPredictorWithBagOfWords_01):\n",
    " \n",
    "    def get_targets(self, data):\n",
    "        ''' create column vector with target labels\n",
    "        '''\n",
    "        # prepare target vector\n",
    "        targets = numpy.zeros(len(data), dtype=numpy.int8)\n",
    "        index = 0\n",
    "        for _, label in data:\n",
    "            if label == 'pos_negation':\n",
    "                targets[index] = 1\n",
    "            index += 1\n",
    "        return targets\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The first step is to retry the original Baseline Naive Bayes set up with this added Negation Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "class PolarityPredictorBowNB(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train NB\n",
    "        self.model = MultinomialNB()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos_negation')\n",
    "            else:\n",
    "                labels.append('neg_negation')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first functionality test\n",
    "\n",
    "model = PolarityPredictorBowNB()\n",
    "model.train(splits[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 pos_negation neg_negation films|adapted|from|comic|books|have|had|plenty|of|success|,|whether\n",
      "   1 pos_negation pos_negation every|now|and|then|a|movie|comes|along|from|a|suspect|studio|,|with\n",
      "   2 pos_negation neg_negation you've|got|mail|works|alot|better|than|it|deserves|to|.|in|order|to|make\n",
      "   3 pos_negation pos_negation \"|jaws|\"|is|a|rare|film|that|grabs|your|attention|before|it|shows|you|a\n",
      "   4 pos_negation neg_negation moviemaking|is|a|lot|like|being|the|general|manager|of|an|nfl|team|in\n",
      "   5 pos_negation pos_negation on|june|30|,|1960|,|a|self-taught|,|idealistic|,|yet|pragmatic|,|young\n",
      "   6 pos_negation pos_negation apparently|,|director|tony|kaye|had|a|major|battle|with|new|line\n",
      "   7 pos_negation pos_negation one|of|my|colleagues|was|surprised|when|i|told|her|i|was|willing|to|see\n",
      "   8 pos_negation pos_negation after|bloody|clashes|and|independence|won|,|lumumba|refused|to|pander|to\n",
      "   9 pos_negation pos_negation the|american|action|film|has|been|slowly|drowning|to|death|in|a|sea|of\n",
      "  10 pos_negation pos_negation after|watching|\"|rat|race|\"|last|week|,|i|noticed|my|cheeks|were|sore\n",
      "  11 pos_negation pos_negation i've|noticed|something|lately|that|i've|never|thought|of|before|.\n"
     ]
    }
   ],
   "source": [
    "def print_first_predictions(model, te_data, n = 12):\n",
    "    predictions = model.predict(te_data)\n",
    "    for i in range(n):\n",
    "        document, label = te_data[i]\n",
    "        prediction = predictions[i]\n",
    "        print('%4d %s %s %s' %(\n",
    "            i, label, prediction,\n",
    "            get_document_preview(document),\n",
    "        ))\n",
    "    \n",
    "print_first_predictions(model, splits[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n",
      "[[82 18]\n",
      " [19 81]]\n"
     ]
    }
   ],
   "source": [
    "labels, accuracy, confusion_matrix = model.predict(\n",
    "    splits[0][1], get_accuracy = True, get_confusion_matrix = True\n",
    ")\n",
    "\n",
    "print(accuracy)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.815\n",
      "Precision --> 0.82\n",
      "Recall --> 0.8118811881188119\n",
      "F1 --> 0.8159203980099502\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.825\n",
      "Precision --> 0.85\n",
      "Recall --> 0.8095238095238095\n",
      "F1 --> 0.8292682926829269\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.83\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8235294117647058\n",
      "F1 --> 0.8316831683168315\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.805\n",
      "Precision --> 0.82\n",
      "Recall --> 0.7961165048543689\n",
      "F1 --> 0.8078817733990147\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.805\n",
      "Precision --> 0.81\n",
      "Recall --> 0.801980198019802\n",
      "F1 --> 0.8059701492537314\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.82\n",
      "Precision --> 0.82\n",
      "Recall --> 0.82\n",
      "F1 --> 0.82\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.86\n",
      "Recall --> 0.8349514563106796\n",
      "F1 --> 0.8472906403940887\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.83\n",
      "Precision --> 0.87\n",
      "Recall --> 0.8055555555555556\n",
      "F1 --> 0.8365384615384616\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.785\n",
      "Precision --> 0.8\n",
      "Recall --> 0.7766990291262136\n",
      "F1 --> 0.788177339901478\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.855\n",
      "Precision --> 0.87\n",
      "Recall --> 0.8446601941747572\n",
      "F1 --> 0.8571428571428571\n",
      "\n",
      "(0.8239873080639339, 0.01953040453766665, 0.788177339901478, 0.8571428571428571)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, splits, verbose = False):\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    fold = 0\n",
    "    for tr_data, te_data in splits:\n",
    "        if verbose:\n",
    "            print('Evaluating fold %d of %d' %(fold+1, len(splits)))\n",
    "            fold += 1\n",
    "        model.train(tr_data)\n",
    "        _, accuracy, confusion_matrix = model.predict(te_data, get_accuracy = True, get_confusion_matrix = True)\n",
    "        \n",
    "        tp, fp, fn, tn = confusion_matrix[0][0], confusion_matrix[0][1], confusion_matrix[1][0], confusion_matrix[1][1]\n",
    "        prec = tp/(tp + fp)\n",
    "        rec = tp/(tp + fn)\n",
    "        f1 = (2*prec*rec)/(prec+rec)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "        if verbose:\n",
    "            print('Accuracy -->', accuracy)\n",
    "            print('Precision -->', prec)\n",
    "            print('Recall -->', rec)\n",
    "            print('F1 -->', f1)\n",
    "            print()\n",
    "    n = float(len(accuracies))\n",
    "    avg = sum(f1s) / n\n",
    "    mse = sum([(x-avg)**2 for x in accuracies]) / n\n",
    "    return (avg, mse**0.5, min(f1s),\n",
    "            max(f1s))\n",
    "\n",
    "# this takes about 3 minutes\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score for this set up was **0.824** which is actually a slight drop off from the baseline Naive Bayes, indicating that negation handling hasn't helped us here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "class PolarityPredictorBowLR(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos_negation')\n",
    "            else:\n",
    "                labels.append('neg_negation')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8484848484848485\n",
      "F1 --> 0.8442211055276383\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.875\n",
      "Precision --> 0.91\n",
      "Recall --> 0.8504672897196262\n",
      "F1 --> 0.8792270531400966\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.83\n",
      "Precision --> 0.86\n",
      "Recall --> 0.8113207547169812\n",
      "F1 --> 0.8349514563106797\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.875\n",
      "Precision --> 0.85\n",
      "Recall --> 0.8947368421052632\n",
      "F1 --> 0.8717948717948718\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.83\n",
      "Recall --> 0.8645833333333334\n",
      "F1 --> 0.8469387755102041\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.86\n",
      "Precision --> 0.88\n",
      "Recall --> 0.8461538461538461\n",
      "F1 --> 0.8627450980392156\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.885\n",
      "Precision --> 0.87\n",
      "Recall --> 0.8969072164948454\n",
      "F1 --> 0.883248730964467\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.86\n",
      "Precision --> 0.84\n",
      "Recall --> 0.875\n",
      "F1 --> 0.8571428571428572\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.84\n",
      "Precision --> 0.88\n",
      "Recall --> 0.8148148148148148\n",
      "F1 --> 0.8461538461538461\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.91\n",
      "Precision --> 0.94\n",
      "Recall --> 0.8867924528301887\n",
      "F1 --> 0.912621359223301\n",
      "\n",
      "(0.8639045153807177, 0.022623398243278036, 0.8349514563106797, 0.912621359223301)\n"
     ]
    }
   ],
   "source": [
    "model = PolarityPredictorBowLR()\n",
    "\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score is **0.864** which is, again, a slight drop off from Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class PolarityPredictorBowDT(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = DecisionTreeClassifier()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.575\n",
      "Precision --> 0.58\n",
      "Recall --> 0.5742574257425742\n",
      "F1 --> 0.5771144278606964\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.64\n",
      "Precision --> 0.67\n",
      "Recall --> 0.6320754716981132\n",
      "F1 --> 0.6504854368932038\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.685\n",
      "Precision --> 0.71\n",
      "Recall --> 0.6761904761904762\n",
      "F1 --> 0.6926829268292682\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.58\n",
      "Precision --> 0.61\n",
      "Recall --> 0.5754716981132075\n",
      "F1 --> 0.5922330097087378\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.645\n",
      "Precision --> 0.64\n",
      "Recall --> 0.6464646464646465\n",
      "F1 --> 0.6432160804020101\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.565\n",
      "Precision --> 0.55\n",
      "Recall --> 0.5670103092783505\n",
      "F1 --> 0.5583756345177665\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.59\n",
      "Precision --> 0.56\n",
      "Recall --> 0.5957446808510638\n",
      "F1 --> 0.577319587628866\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.665\n",
      "Precision --> 0.63\n",
      "Recall --> 0.6774193548387096\n",
      "F1 --> 0.6528497409326425\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.635\n",
      "Precision --> 0.62\n",
      "Recall --> 0.6391752577319587\n",
      "F1 --> 0.6294416243654821\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.69\n",
      "Precision --> 0.69\n",
      "Recall --> 0.69\n",
      "F1 --> 0.69\n",
      "\n",
      "(0.6263718469138674, 0.0441179620596829, 0.5583756345177665, 0.6926829268292682)\n"
     ]
    }
   ],
   "source": [
    "model = PolarityPredictorBowDT()\n",
    "\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score for Decision Tree with Negation Handling was slightly better than Baseline Decision Tree but still poor compared with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "class PolarityPredictorBowSVM(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = svm.SVC()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.89\n",
      "Recall --> 0.8240740740740741\n",
      "F1 --> 0.8557692307692307\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.835\n",
      "Precision --> 0.9\n",
      "Recall --> 0.7964601769911505\n",
      "F1 --> 0.8450704225352113\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.82\n",
      "Precision --> 0.87\n",
      "Recall --> 0.7909090909090909\n",
      "F1 --> 0.8285714285714286\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.86\n",
      "Precision --> 0.85\n",
      "Recall --> 0.8673469387755102\n",
      "F1 --> 0.8585858585858585\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.85\n",
      "Recall --> 0.85\n",
      "F1 --> 0.85\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.855\n",
      "Precision --> 0.87\n",
      "Recall --> 0.8446601941747572\n",
      "F1 --> 0.8571428571428571\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.905\n",
      "Precision --> 0.9\n",
      "Recall --> 0.9090909090909091\n",
      "F1 --> 0.9045226130653266\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.885\n",
      "Precision --> 0.89\n",
      "Recall --> 0.8811881188118812\n",
      "F1 --> 0.8855721393034827\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.85\n",
      "Recall --> 0.85\n",
      "F1 --> 0.85\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.885\n",
      "Precision --> 0.92\n",
      "Recall --> 0.8598130841121495\n",
      "F1 --> 0.888888888888889\n",
      "\n",
      "(0.8624123438862282, 0.024304973707281644, 0.8285714285714286, 0.9045226130653266)\n"
     ]
    }
   ],
   "source": [
    "model = PolarityPredictorBowSVM()\n",
    "\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score here was **0.8624** which is a small increase vs. Baseline SVM. Once more, however, this SVM set up took many hours to train and so may not be the best choice even with a solid F1 score.\n",
    "\n",
    "At this point it isn't clear if Negation Handling is an overall improvement. It seemed to improve some algorithms but not others.\n",
    "\n",
    "The next step was to use Bigrams instead of Unigrams when training the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
