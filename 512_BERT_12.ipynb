{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT with 512 Max Sequence Length and 12 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next configuration, I have 512 as the max sequence length and the number of epochs has been doubled from 6 to 12. This was another large jump in training time and it took many hours to complete the 10 CV folds. Once more I saved the ten so I could use the performance evaluation functions that I had written to see how the model performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_512_12_files = []\n",
    "    \n",
    "for i in range(1,11):\n",
    "    bert_512_12_files.append(f'512_12_BERT/{i}_pred_512_12.txt')    \n",
    "\n",
    "    \n",
    "c_names = ['gold','pred','correct','text']\n",
    "    \n",
    "df1_512_12 = pd.DataFrame(columns=c_names)\n",
    "df2_512_12 = pd.DataFrame(columns=c_names)\n",
    "df3_512_12 = pd.DataFrame(columns=c_names)\n",
    "df4_512_12 = pd.DataFrame(columns=c_names)\n",
    "df5_512_12 = pd.DataFrame(columns=c_names)\n",
    "df6_512_12 = pd.DataFrame(columns=c_names)\n",
    "df7_512_12 = pd.DataFrame(columns=c_names)\n",
    "df8_512_12 = pd.DataFrame(columns=c_names)\n",
    "df9_512_12 = pd.DataFrame(columns=c_names)\n",
    "df10_512_12 = pd.DataFrame(columns=c_names)\n",
    "\n",
    "dataframes_512_12 = [df1_512_12,df2_512_12,df3_512_12,df4_512_12,df5_512_12,df6_512_12,df7_512_12,df8_512_12,df9_512_12,df10_512_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs(files, df_list):\n",
    "    j = 0\n",
    "    for dataframe in df_list:\n",
    "\n",
    "        #dataframe = pd.DataFrame(columns=['index','gold','pred','correct','text'])\n",
    "        processed_lines = []\n",
    "\n",
    "        with open(files[j], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            count = 0\n",
    "            for line in lines[1:]:\n",
    "                tokens = line.split()\n",
    "                line_length = len(tokens)\n",
    "                temp_line = ''\n",
    "\n",
    "                for i in range(4, (line_length)):\n",
    "                    temp_line = temp_line + tokens[i] + ' '\n",
    "\n",
    "                processed_line = [tokens[1],tokens[2],tokens[3], temp_line]\n",
    "                processed_lines.append(processed_line)\n",
    "                dataframe.loc[count] = processed_line\n",
    "                count+=1\n",
    "        j+=1\n",
    "    return(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_512_12 = create_dfs(bert_512_12_files, dataframes_512_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(dataframe):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    corrects = 0\n",
    "    errors = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        if dataframe.iat[i,2] == 'yes':\n",
    "            corrects += 1\n",
    "        else:\n",
    "            errors.append(i)\n",
    "        if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "            true_pos += 1\n",
    "        elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "            false_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "            true_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "            false_pos += 1\n",
    "    \n",
    "    accuracy = corrects/len(dataframe)\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    f1_score = 2*((precision*recall)/(precision + recall))\n",
    "    return(accuracy,precision,recall,f1_score,errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averages(df_list):\n",
    "    accuracies = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    errors_list = []\n",
    "    for dataframe in df_list:    \n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "        corrects = 0\n",
    "        errors = []\n",
    "        for i in range(0,len(dataframe)):\n",
    "            if dataframe.iat[i,2] == 'yes':\n",
    "                corrects += 1\n",
    "            else:\n",
    "                errors.append(i)\n",
    "            if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "                true_pos += 1\n",
    "            elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "                false_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "                true_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "                false_pos += 1\n",
    "\n",
    "        accuracy = corrects/len(dataframe)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "        precs.append(precision)\n",
    "\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "        recs.append(recall)\n",
    "        \n",
    "        f1_score = 2*((precision*recall)/(precision + recall))\n",
    "        f1s.append(f1_score)\n",
    "        \n",
    "        errors_list.append(errors)\n",
    "        \n",
    "    return(sum(accuracies)/len(df_list),sum(precs)/len(df_list),sum(recs)/len(df_list),sum(f1s)/len(df_list), errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_averages_get_errors(dataframes, errorlist = False):\n",
    "    acc,prec,rec,f1,errors = get_averages(dataframes)\n",
    "    if errorlist == True:\n",
    "        return(errors)\n",
    "    else:\n",
    "        for i, dataframe in enumerate(dataframes):\n",
    "            scores = get_f1(dataframe)\n",
    "            print(f'Cross validation {i+1}')\n",
    "            print(f'The accuracy is {scores[0]*100:.2f}%')\n",
    "            print(f'The precision is {scores[1]*100:.2f}%')\n",
    "            print(f'The recall is {scores[2]*100:.2f}%')\n",
    "            print(f'The F1 score is {scores[3]*100:.2f}%')\n",
    "            print(f'The model got the following rows wrong {scores[4]}\\n')\n",
    "\n",
    "        print(f'The average accuracy is {acc*100:.2f}%')\n",
    "        print(f'The average precision is {prec*100:.2f}%')\n",
    "        print(f'The average recall is {rec*100:.2f}%')\n",
    "        print(f'The average F1 score is {f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 1\n",
      "The accuracy is 93.50%\n",
      "The precision is 95.79%\n",
      "The recall is 91.00%\n",
      "The F1 score is 93.33%\n",
      "The model got the following rows wrong [1, 4, 10, 24, 25, 44, 50, 82, 91, 118, 135, 157, 171]\n",
      "\n",
      "Cross validation 2\n",
      "The accuracy is 92.50%\n",
      "The precision is 92.08%\n",
      "The recall is 93.00%\n",
      "The F1 score is 92.54%\n",
      "The model got the following rows wrong [9, 18, 59, 62, 88, 93, 94, 104, 115, 125, 137, 142, 167, 177, 189]\n",
      "\n",
      "Cross validation 3\n",
      "The accuracy is 93.00%\n",
      "The precision is 90.57%\n",
      "The recall is 96.00%\n",
      "The F1 score is 93.20%\n",
      "The model got the following rows wrong [8, 30, 50, 99, 100, 107, 111, 121, 133, 137, 142, 156, 162, 178]\n",
      "\n",
      "Cross validation 4\n",
      "The accuracy is 92.50%\n",
      "The precision is 92.93%\n",
      "The recall is 92.00%\n",
      "The F1 score is 92.46%\n",
      "The model got the following rows wrong [7, 14, 21, 34, 70, 83, 94, 98, 105, 109, 120, 128, 131, 159, 181]\n",
      "\n",
      "Cross validation 5\n",
      "The accuracy is 88.50%\n",
      "The precision is 89.69%\n",
      "The recall is 87.00%\n",
      "The F1 score is 88.32%\n",
      "The model got the following rows wrong [0, 19, 20, 26, 30, 35, 36, 37, 51, 55, 67, 77, 98, 100, 109, 130, 144, 155, 162, 172, 189, 192, 199]\n",
      "\n",
      "Cross validation 6\n",
      "The accuracy is 93.50%\n",
      "The precision is 97.80%\n",
      "The recall is 89.00%\n",
      "The F1 score is 93.19%\n",
      "The model got the following rows wrong [2, 7, 34, 39, 41, 45, 63, 80, 85, 90, 99, 133, 171]\n",
      "\n",
      "Cross validation 7\n",
      "The accuracy is 90.50%\n",
      "The precision is 89.32%\n",
      "The recall is 92.00%\n",
      "The F1 score is 90.64%\n",
      "The model got the following rows wrong [6, 27, 32, 42, 43, 44, 49, 93, 100, 102, 109, 117, 118, 126, 145, 171, 177, 185, 197]\n",
      "\n",
      "Cross validation 8\n",
      "The accuracy is 90.00%\n",
      "The precision is 87.74%\n",
      "The recall is 93.00%\n",
      "The F1 score is 90.29%\n",
      "The model got the following rows wrong [35, 58, 61, 74, 89, 90, 92, 104, 109, 119, 129, 131, 132, 134, 135, 142, 169, 185, 196, 197]\n",
      "\n",
      "Cross validation 9\n",
      "The accuracy is 91.50%\n",
      "The precision is 96.63%\n",
      "The recall is 86.00%\n",
      "The F1 score is 91.01%\n",
      "The model got the following rows wrong [16, 19, 25, 40, 41, 50, 55, 56, 59, 76, 81, 82, 84, 93, 101, 103, 145]\n",
      "\n",
      "Cross validation 10\n",
      "The accuracy is 95.00%\n",
      "The precision is 95.92%\n",
      "The recall is 94.00%\n",
      "The F1 score is 94.95%\n",
      "The model got the following rows wrong [7, 29, 32, 40, 47, 63, 112, 121, 127, 141]\n",
      "\n",
      "The average accuracy is 92.05%\n",
      "The average precision is 92.85%\n",
      "The average recall is 91.30%\n",
      "The average F1 score is 91.99%\n"
     ]
    }
   ],
   "source": [
    "print_averages_get_errors(dataframes_512_12, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy has jumped once again, from 91.15% to 92.05%. This isnt as big an increase but it's still almost 1 full percentage point, and this configuration ultimately turned out to be the very best performing one across both assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
