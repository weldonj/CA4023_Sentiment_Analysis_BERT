{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT with 512 Max Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adjusted the provided model to use a max sequence length of 512 instead of the default 256. This added some significant time to the training process. I saved the predictions from the 10 CV folds so I could repeat the performance evaluation from the previous notebook. These files were each roughly twice as large as the ones from the 256 version (220KB to 430KB) which was to be expected, having roughly twice as much text in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_512_files = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    bert_512_files.append(f'512_BERT/{i}_pred_512.txt')\n",
    "    \n",
    "c_names = ['gold','pred','correct','text']\n",
    "    \n",
    "df1_512 = pd.DataFrame(columns=c_names)\n",
    "df2_512 = pd.DataFrame(columns=c_names)\n",
    "df3_512 = pd.DataFrame(columns=c_names)\n",
    "df4_512 = pd.DataFrame(columns=c_names)\n",
    "df5_512 = pd.DataFrame(columns=c_names)\n",
    "df6_512 = pd.DataFrame(columns=c_names)\n",
    "df7_512 = pd.DataFrame(columns=c_names)\n",
    "df8_512 = pd.DataFrame(columns=c_names)\n",
    "df9_512 = pd.DataFrame(columns=c_names)\n",
    "df10_512 = pd.DataFrame(columns=c_names)\n",
    "\n",
    "dataframes_512 = [df1_512,df2_512,df3_512,df4_512,df5_512,df6_512,df7_512,df8_512,df9_512,df10_512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs(files, df_list):\n",
    "    j = 0\n",
    "    for dataframe in df_list:\n",
    "\n",
    "        #dataframe = pd.DataFrame(columns=['index','gold','pred','correct','text'])\n",
    "        processed_lines = []\n",
    "\n",
    "        with open(files[j], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            count = 0\n",
    "            for line in lines[1:]:\n",
    "                tokens = line.split()\n",
    "                line_length = len(tokens)\n",
    "                temp_line = ''\n",
    "\n",
    "                for i in range(4, (line_length)):\n",
    "                    temp_line = temp_line + tokens[i] + ' '\n",
    "\n",
    "                processed_line = [tokens[1],tokens[2],tokens[3], temp_line]\n",
    "                processed_lines.append(processed_line)\n",
    "                dataframe.loc[count] = processed_line\n",
    "                count+=1\n",
    "        j+=1\n",
    "    return(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_512 = create_dfs(bert_512_files, dataframes_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(dataframe):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    corrects = 0\n",
    "    errors = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        if dataframe.iat[i,2] == 'yes':\n",
    "            corrects += 1\n",
    "        else:\n",
    "            errors.append(i)\n",
    "        if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "            true_pos += 1\n",
    "        elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "            false_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "            true_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "            false_pos += 1\n",
    "    \n",
    "    accuracy = corrects/len(dataframe)\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    f1_score = 2*((precision*recall)/(precision + recall))\n",
    "    return(accuracy,precision,recall,f1_score,errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averages(df_list):\n",
    "    accuracies = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    errors_list = []\n",
    "    for dataframe in df_list:    \n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "        corrects = 0\n",
    "        errors = []\n",
    "        for i in range(0,len(dataframe)):\n",
    "            if dataframe.iat[i,2] == 'yes':\n",
    "                corrects += 1\n",
    "            else:\n",
    "                errors.append(i)\n",
    "            if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "                true_pos += 1\n",
    "            elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "                false_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "                true_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "                false_pos += 1\n",
    "\n",
    "        accuracy = corrects/len(dataframe)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "        precs.append(precision)\n",
    "\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "        recs.append(recall)\n",
    "        \n",
    "        f1_score = 2*((precision*recall)/(precision + recall))\n",
    "        f1s.append(f1_score)\n",
    "        \n",
    "        errors_list.append(errors)\n",
    "        \n",
    "    return(sum(accuracies)/len(df_list),sum(precs)/len(df_list),sum(recs)/len(df_list),sum(f1s)/len(df_list), errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_averages_get_errors(dataframes, errorlist = False):\n",
    "    acc,prec,rec,f1,errors = get_averages(dataframes)\n",
    "    if errorlist == True:\n",
    "        return(errors)\n",
    "    else:\n",
    "        for i, dataframe in enumerate(dataframes):\n",
    "            scores = get_f1(dataframe)\n",
    "            print(f'Cross validation {i+1}')\n",
    "            print(f'The accuracy is {scores[0]*100:.2f}%')\n",
    "            print(f'The precision is {scores[1]*100:.2f}%')\n",
    "            print(f'The recall is {scores[2]*100:.2f}%')\n",
    "            print(f'The F1 score is {scores[3]*100:.2f}%')\n",
    "            print(f'The model got the following rows wrong {scores[4]}\\n')\n",
    "\n",
    "        print(f'The average accuracy is {acc*100:.2f}%')\n",
    "        print(f'The average precision is {prec*100:.2f}%')\n",
    "        print(f'The average recall is {rec*100:.2f}%')\n",
    "        print(f'The average F1 score is {f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 1\n",
      "The accuracy is 90.00%\n",
      "The precision is 90.82%\n",
      "The recall is 89.00%\n",
      "The F1 score is 89.90%\n",
      "The model got the following rows wrong [1, 10, 24, 25, 40, 44, 50, 56, 82, 83, 91, 106, 118, 134, 135, 136, 157, 159, 171, 173]\n",
      "\n",
      "Cross validation 2\n",
      "The accuracy is 93.50%\n",
      "The precision is 91.43%\n",
      "The recall is 96.00%\n",
      "The F1 score is 93.66%\n",
      "The model got the following rows wrong [9, 59, 88, 94, 104, 115, 125, 137, 142, 146, 167, 177, 189]\n",
      "\n",
      "Cross validation 3\n",
      "The accuracy is 92.00%\n",
      "The precision is 89.62%\n",
      "The recall is 95.00%\n",
      "The F1 score is 92.23%\n",
      "The model got the following rows wrong [8, 30, 50, 62, 99, 100, 107, 121, 128, 133, 142, 147, 156, 162, 173, 178]\n",
      "\n",
      "Cross validation 4\n",
      "The accuracy is 87.50%\n",
      "The precision is 87.88%\n",
      "The recall is 87.00%\n",
      "The F1 score is 87.44%\n",
      "The model got the following rows wrong [7, 14, 21, 27, 28, 30, 34, 39, 70, 83, 85, 94, 98, 108, 109, 120, 121, 128, 131, 133, 159, 163, 179, 181, 191]\n",
      "\n",
      "Cross validation 5\n",
      "The accuracy is 87.50%\n",
      "The precision is 88.66%\n",
      "The recall is 86.00%\n",
      "The F1 score is 87.31%\n",
      "The model got the following rows wrong [0, 18, 19, 20, 26, 30, 35, 36, 37, 51, 55, 67, 77, 98, 100, 109, 130, 131, 144, 155, 162, 172, 189, 192, 199]\n",
      "\n",
      "Cross validation 6\n",
      "The accuracy is 93.50%\n",
      "The precision is 93.94%\n",
      "The recall is 93.00%\n",
      "The F1 score is 93.47%\n",
      "The model got the following rows wrong [2, 4, 7, 34, 79, 85, 99, 119, 124, 133, 143, 154, 171]\n",
      "\n",
      "Cross validation 7\n",
      "The accuracy is 90.50%\n",
      "The precision is 87.16%\n",
      "The recall is 95.00%\n",
      "The F1 score is 90.91%\n",
      "The model got the following rows wrong [6, 32, 43, 44, 93, 100, 102, 109, 117, 118, 122, 126, 145, 147, 150, 155, 171, 185, 197]\n",
      "\n",
      "Cross validation 8\n",
      "The accuracy is 89.00%\n",
      "The precision is 86.11%\n",
      "The recall is 93.00%\n",
      "The F1 score is 89.42%\n",
      "The model got the following rows wrong [21, 35, 58, 74, 89, 90, 92, 104, 108, 109, 119, 122, 129, 131, 132, 134, 135, 142, 169, 185, 196, 197]\n",
      "\n",
      "Cross validation 9\n",
      "The accuracy is 92.00%\n",
      "The precision is 94.68%\n",
      "The recall is 89.00%\n",
      "The F1 score is 91.75%\n",
      "The model got the following rows wrong [10, 16, 25, 41, 55, 56, 59, 76, 81, 82, 93, 101, 103, 105, 145, 186]\n",
      "\n",
      "Cross validation 10\n",
      "The accuracy is 96.00%\n",
      "The precision is 98.94%\n",
      "The recall is 93.00%\n",
      "The F1 score is 95.88%\n",
      "The model got the following rows wrong [7, 11, 29, 32, 36, 47, 63, 112]\n",
      "\n",
      "The average accuracy is 91.15%\n",
      "The average precision is 90.92%\n",
      "The average recall is 91.60%\n",
      "The average F1 score is 91.20%\n"
     ]
    }
   ],
   "source": [
    "print_averages_get_errors(dataframes_512, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this jump from 256 tokens to 512 tokens has given a nice immediate improvement to the average accuracy. We have gone from 89.3% to 91.15%. A not insignificant increase. The next thing I wanted to do was to redo the 512 max sequence length, but double the epoch count to 12. This is what is detailed in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}