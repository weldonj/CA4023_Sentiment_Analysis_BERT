{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76544846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# choose between 'local-tgz' and 'web',\n",
    "# see description under each heading below\n",
    "\n",
    "data_source = 'local-tgz'\n",
    "\n",
    "# adjust path as needed\n",
    "\n",
    "data_tgz = os.path.join('data', 'pang-and-lee-2004', 'data.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7d03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import defaultdict\n",
    "\n",
    "class PL04DataLoader_Part_1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_labelled_dataset(self, fold = 0):\n",
    "        ''' Compile a fold of the data set\n",
    "        '''\n",
    "        dataset = []\n",
    "        for label in ('pos', 'neg'):\n",
    "            for document in self.get_documents(\n",
    "                fold = fold,\n",
    "                label = label,\n",
    "            ):\n",
    "                dataset.append((document, label))\n",
    "        return dataset\n",
    "    \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        ''' Enumerate the raw contents of selected data set files.\n",
    "            Args:\n",
    "                data_dir: relative or absolute path to the data set folder\n",
    "                fold: which fold to load (0 to n_folds-1)\n",
    "                label: 'pos' or 'neg' to\n",
    "                    select data with positive or negative sentiment\n",
    "                    polarity\n",
    "            Return:\n",
    "                List of tokenised documents, each a list of sentences\n",
    "                that in turn are lists of tokens\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "487effb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL04DataLoader(PL04DataLoader_Part_1):\n",
    "    \n",
    "    def get_xval_splits(self):\n",
    "        ''' Split data with labels for cross-validation\n",
    "            returns a list of k pairs (training_data, test_data)\n",
    "            for k cross-validation\n",
    "        '''\n",
    "        # load the folds\n",
    "        folds = []\n",
    "        for i in range(10):\n",
    "            folds.append(self.get_labelled_dataset(\n",
    "                fold = i\n",
    "            ))\n",
    "        # create training-test splits\n",
    "        retval = []\n",
    "        for i in range(10):\n",
    "            test_data = folds[i]\n",
    "            training_data = []\n",
    "            for j in range(9):\n",
    "                ij1 = (i+j+1) % 10\n",
    "                assert ij1 != i\n",
    "                training_data = training_data + folds[ij1]\n",
    "            retval.append((training_data, test_data))\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee644f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import time\n",
    "\n",
    "class PL04DataLoaderFromStream(PL04DataLoader):\n",
    "        \n",
    "    def __init__(self, tgz_stream, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        with tarfile.open(\n",
    "            mode = 'r|gz',\n",
    "            fileobj = tgz_stream\n",
    "        ) as tar_archive:\n",
    "            for tar_member in tar_archive:\n",
    "                if counter == 2000:\n",
    "                    break\n",
    "                path_components = tar_member.name.split('/')\n",
    "                filename = path_components[-1]\n",
    "                if filename.startswith('cv') \\\n",
    "                and filename.endswith('.txt') \\\n",
    "                and '_' in filename:\n",
    "                    label = path_components[-2]\n",
    "                    fold = int(filename[2])\n",
    "                    key = (fold, label)\n",
    "                    if key not in self.data:\n",
    "                        self.data[key] = []\n",
    "                    f = tar_archive.extractfile(tar_member)\n",
    "                    document = [\n",
    "                        line.decode('utf-8').split()\n",
    "                        for line in f.readlines()\n",
    "                    ]\n",
    "                    self.data[key].append(document)\n",
    "                    counter += 1\n",
    "            \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        return self.data[(fold, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c77111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL04DataLoaderFromTGZ(PL04DataLoaderFromStream):\n",
    "    \n",
    "    def __init__(self, data_path, **kwargs):\n",
    "        with open(data_path, 'rb') as tgz_stream:\n",
    "            super().__init__(tgz_stream, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0815f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# adjust the torch version below following instructions on https://pytorch.org/get-started/locally/\n",
    "\n",
    "import sys\n",
    "\n",
    "# for why we use {sys.executable} see\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install transformers\n",
    "\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-lightning\n",
    "\n",
    "try:\n",
    "    import torchnlp\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install pytorch-nlp\n",
    "\n",
    "try:\n",
    "    import tensorboard\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install tensorboard\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745405bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87488e6dd56444fa9e37985c2aa3a3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc202e6699949868b2102d907a2292f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d054f3b6b03474ebde9f5345f6cc7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4684899ad34a39838c5bbc910001df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "model_size = 'base'  # choose between 'tiny', 'base' and 'large'\n",
    "\n",
    "size2name = {\n",
    "    'tiny':  'distilbert-base-uncased',  # TODO: This doesn't seem to reduce memory usage compared to bert-base. \n",
    "    'base':  'bert-base-uncased',\n",
    "    'large': 'bert-large-uncased',\n",
    "}\n",
    "\n",
    "\n",
    "model_name = size2name[model_size]\n",
    "\n",
    "force_whitespace_pre_tokeniser = False  # should not matter with pre-tokenised P&L'04 input\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "if force_whitespace_pre_tokeniser:\n",
    "    tokeniser.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "775ca8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t['input_ids']: [101, 7592, 2088, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'hello', 'world', '!', '[SEP]']\n",
      "\t.word_ids():   [None, 0, 1, 2, None]\n",
      "             hello --> [7592]\n",
      "             world --> [2088]\n",
      "                 ! --> [999]\n",
      "1 \t['input_ids']: [101, 19204, 6648, 1005, 1055, 4569, 102]\n",
      "\ttokens:        ['[CLS]', 'token', '##isation', \"'\", 's', 'fun', '[SEP]']\n",
      "\t.word_ids():   [None, 0, 0, 1, 1, 2, None]\n",
      "      tokenisation --> [19204, 6648]\n",
      "                's --> [1005, 1055]\n",
      "               fun --> [4569]\n"
     ]
    }
   ],
   "source": [
    "# let's test the tokeniser with a small example\n",
    "\n",
    "example_batch = [\n",
    "    'hello world !'.split(),\n",
    "    \"\"\"tokenisation 's fun\"\"\".split(),\n",
    "]\n",
    "\n",
    "tokenised_text = tokeniser(\n",
    "    example_batch,  # pre-tokenised input\n",
    "    is_split_into_words = True,\n",
    ")\n",
    "\n",
    "for i, token_ids in enumerate(tokenised_text['input_ids']):\n",
    "    print(i, \"\\t['input_ids']:\", token_ids)\n",
    "    print(   '\\ttokens:       ', tokeniser.convert_ids_to_tokens(token_ids))\n",
    "    print(   \"\\t.word_ids():  \", tokenised_text.word_ids(batch_index = i))\n",
    "    example = example_batch[i]\n",
    "    for token in example:\n",
    "        # https://stackoverflow.com/questions/62317723/tokens-to-words-mapping-in-the-tokenizer-decode-step-huggingface\n",
    "        print('%18s --> %r' %(\n",
    "            token,\n",
    "            tokeniser.encode(\n",
    "                token,\n",
    "                add_special_tokens = False,\n",
    "            )\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6711ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have sufficient TPU or GPU memory you can increase the sequence length up to 512\n",
    "# (memory requirements will be the highest during fine-tuning of BERT)\n",
    "max_sequence_length  = 256\n",
    "\n",
    "# memory requirements increase linearly with the batch size;\n",
    "# a batch size of 16, better 32, is needed for efficient training;\n",
    "# however, with little memory, we have to go lower\n",
    "batch_size      = 10   # for a 12 GB card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc330883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (926 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LengthBin\tPos\tNeg\tTotal\n",
      "   0- 249\t7\t13\t20\n",
      " 250- 499\t126\t148\t274\n",
      " 500- 749\t271\t317\t588\n",
      " 750- 999\t275\t298\t573\n",
      "1000-1249\t171\t136\t307\n",
      "1250-1499\t75\t49\t124\n",
      "1500-1749\t37\t20\t57\n",
      "1750-1999\t21\t10\t31\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "bin_width = 250\n",
    "data_loader = PL04DataLoaderFromTGZ('data.tar.gz')\n",
    "distribution = defaultdict(lambda: 0)\n",
    "\n",
    "# interate all reviews\n",
    "for fold in range(10):\n",
    "    for label in 'pos neg'.split():\n",
    "        batch = []\n",
    "        for document in data_loader.get_documents(\n",
    "            label = label,\n",
    "            fold = fold,\n",
    "        ):\n",
    "            tokens = []\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    tokens.append(token)\n",
    "            batch.append(tokens)\n",
    "        tokenised_batch = tokeniser(\n",
    "            batch,  # pre-tokenised input\n",
    "            is_split_into_words = True,\n",
    "        )\n",
    "        max_length_bin = 0\n",
    "        for token_ids in tokenised_batch['input_ids']:\n",
    "            length = len(token_ids)\n",
    "            length_bin = length // bin_width\n",
    "            distribution[(label,   length_bin)] += 1\n",
    "            distribution[('total', length_bin)] += 1\n",
    "            if length_bin > max_length_bin:\n",
    "                max_length_bin = length_bin\n",
    "print('LengthBin\\tPos\\tNeg\\tTotal')\n",
    "for length_bin in range(0, max_length_bin+1):\n",
    "    row = []\n",
    "    row.append('%4d-%4d' %(\n",
    "        bin_width*length_bin,\n",
    "        bin_width*(1+length_bin)-1\n",
    "    ))\n",
    "    for label in 'pos neg total'.split():\n",
    "        count = distribution[(label, length_bin)]\n",
    "        row.append('%d' %count)        \n",
    "    print('\\t'.join(row))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd9bf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic usage of pytorch and lightning from\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# and\n",
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "    \n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "class SlicedDocuments_part_1(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_data,\n",
    "        tokeniser = None,\n",
    "        fraction_for_first_sequence = 0.5,\n",
    "        max_sequence_length = 128,\n",
    "        second_part_as_sequence_B = False,\n",
    "        preproc_batch_size = 8,\n",
    "        \n",
    "    ):\n",
    "        '''Extracts slices from labelled documents\n",
    "           Args:\n",
    "               raw_data: list of (document, label) pairs\n",
    "               tokeniser: transformer tokeniser to obtain subword units; this tokeniser\n",
    "                   will be used to decide how many words to include in each slice; it\n",
    "                   is recommended to use the same tokeniser that will be used to tokenise\n",
    "                   the data\n",
    "               fraction_for_first_sequence: 1 = take slice from the start of the document,\n",
    "                   0 = take slice from the end of the document, > 0 and < 1 = take two\n",
    "                   slices, one of this relative size from the start and then fill to\n",
    "                   max_sequence_length from the end of the document\n",
    "               max_sequence_length: produce sequences up to this length\n",
    "               allow_partial_tokens: whether to slice between subword units of tokens; if\n",
    "                   set to True the produced sequences will always have max_sequence_length\n",
    "                   items unless the document is very short\n",
    "        '''\n",
    "        assert max_sequence_length >= 5\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.second_part_as_sequence_B = second_part_as_sequence_B\n",
    "        self.init_sequence_lengths(fraction_for_first_sequence)\n",
    "        self.tokeniser = tokeniser\n",
    "        self.init_slices(raw_data, preproc_batch_size)\n",
    "\n",
    "    def init_sequence_lengths(self, fraction_for_first_sequence):\n",
    "        available_for_two_sequences = self.max_sequence_length - 3\n",
    "        self.first_sequence_length = max(0, int(\n",
    "            fraction_for_first_sequence * available_for_two_sequences\n",
    "        ))\n",
    "        self.last_sequence_length = max(\n",
    "            0,\n",
    "            available_for_two_sequences - self.first_sequence_length\n",
    "        )\n",
    "        assert self.first_sequence_length \\\n",
    "            + self.last_sequence_length   \\\n",
    "            <= available_for_two_sequences\n",
    "        if self.first_sequence_length == 0:\n",
    "            self.last_sequence_length += 1\n",
    "        elif self.last_sequence_length == 0:\n",
    "            self.first_sequence_length += 1\n",
    "        if self.first_sequence_length == 0 \\\n",
    "        or self.last_sequence_length == 0:\n",
    "            # do not use a second [SEP] marker when there is\n",
    "            # always only one sequence\n",
    "            self.second_part_as_sequence_B = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e58df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_2(SlicedDocuments_part_1):\n",
    "    \n",
    "    def init_slices(self, raw_data, preproc_batch_size):\n",
    "        self.slices = []\n",
    "        next_batch = []\n",
    "        next_labels = []\n",
    "        for document, label in raw_data:\n",
    "            tokens = []\n",
    "            for sentence in document:\n",
    "                tokens = tokens + sentence\n",
    "            next_batch.append(tokens)    \n",
    "            next_labels.append(label)\n",
    "            if len(next_batch) >= preproc_batch_size:\n",
    "                self.add_batch(next_batch, next_labels)\n",
    "                next_batch = []\n",
    "                next_labels = []\n",
    "        if next_batch:\n",
    "            self.add_batch(next_batch, next_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a5c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_3(SlicedDocuments_part_2):\n",
    "    \n",
    "    def add_batch(self, batch, labels):\n",
    "        # determine, for each document in the batch, how many\n",
    "        # tokens to include from the start of the document\n",
    "        if self.first_sequence_length:\n",
    "            lengths_1 = self.get_lengths(batch)\n",
    "        else:\n",
    "            lengths_1 = len(batch) * [0]\n",
    "        # determine, for each document in the batch, how many\n",
    "        # tokens to include from the end of the document    \n",
    "        if self.last_sequence_length:\n",
    "            lengths_2 = self.get_lengths(batch, part = 2, lengths_1 = lengths_1)\n",
    "        else:\n",
    "            lengths_2 = len(batch) * [0]\n",
    "        # TODO: In an earlier version, we did not check the following\n",
    "        #       condition, creating a second sequence for short\n",
    "        #       documents even though only one sequence is requested.\n",
    "        #       First results indicate that this bug actually\n",
    "        #       improves performance. Future work should investigate\n",
    "        #       this and, if this effect is confirmed, propose a\n",
    "        #       clean solution to exploit this effect.\n",
    "        if self.first_sequence_length:\n",
    "            # sometimes there is space for more tokens from the\n",
    "            # start even though no more from the end fit\n",
    "            lengths_1 = self.expand_lengths(batch, lengths_1, lengths_2)\n",
    "        # TODO: For cases with length_1 + length_2 > len(tokens),\n",
    "        #       should we adjust the parts to not overlap?       \n",
    "        # prepare texts\n",
    "        for batch_idx, tokens in enumerate(batch):\n",
    "            parts = []\n",
    "            length_1 = lengths_1[batch_idx]\n",
    "            length_2 = lengths_2[batch_idx]\n",
    "            part_1 = tokens[:length_1]\n",
    "            if length_2 > 0:\n",
    "                part_2 = tokens[-length_2:]\n",
    "            else:\n",
    "                part_2 = []\n",
    "            if self.second_part_as_sequence_B:\n",
    "                parts.append(part_1)\n",
    "                parts.append(part_2)\n",
    "            else:\n",
    "                parts.append(part_1 + part_2)\n",
    "            assert len(parts) > 0    \n",
    "            self.slices.append((parts, labels[batch_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "754319c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_4(SlicedDocuments_part_3):\n",
    "\n",
    "    def get_lengths(\n",
    "        self, batch, part = 1, lengths_1 = None,\n",
    "        lengths_2 = None,\n",
    "    ):\n",
    "        if part == 3:\n",
    "            lower_limits = lengths_1[:]  # clone\n",
    "        else:\n",
    "            lower_limits = len(batch) * [0]\n",
    "        upper_limits = []\n",
    "        for tokens in batch:\n",
    "            upper_limits.append(min(\n",
    "                len(tokens),\n",
    "                self.max_sequence_length\n",
    "            ))\n",
    "        # we want each upper limit to be a sequence length\n",
    "        # that is too big but sometimes the full document\n",
    "        # (or max_length words) can fit --> test for this\n",
    "        # special case\n",
    "        for batch_idx, fit in enumerate(\n",
    "            self.get_fit(batch, upper_limits, part, lengths_1, lengths_2)\n",
    "        ):\n",
    "            if fit:\n",
    "                # update lower limit to match upper limit\n",
    "                # to mark this document as not needing any\n",
    "                # further length search\n",
    "                lower_limits[batch_idx] = upper_limits[batch_idx]\n",
    "        while True:\n",
    "            # prepare next lengths to test and check whether search is finished\n",
    "            new_limits = []\n",
    "            all_done = True\n",
    "            for batch_idx, tokens in enumerate(batch):\n",
    "                if lower_limits[batch_idx]+1 >= upper_limits[batch_idx]:\n",
    "                    new_limits.append(lower_limits[batch_idx])\n",
    "                else:\n",
    "                    all_done = False\n",
    "                    new_limits.append((\n",
    "                        lower_limits[batch_idx] + upper_limits[batch_idx]\n",
    "                    )//2)\n",
    "            if all_done:\n",
    "                return lower_limits\n",
    "            # adjust lower and upper limits\n",
    "            for batch_idx, fit in enumerate(\n",
    "                self.get_fit(batch, new_limits, part, lengths_1, lengths_2)\n",
    "            ):\n",
    "                if fit:\n",
    "                    lower_limits[batch_idx] = new_limits[batch_idx]\n",
    "                else:\n",
    "                    upper_limits[batch_idx] = new_limits[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78285005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedDocuments_part_5(SlicedDocuments_part_4):\n",
    "\n",
    "    def get_fit(self, batch, limits, part, lengths_1, lengths_2 = None):\n",
    "        sliced_batch_A = []\n",
    "        sliced_batch_B = []\n",
    "        for batch_idx, tokens in enumerate(batch):\n",
    "            if part == 1:\n",
    "                length_1 = limits[batch_idx]\n",
    "                length_2 = 0\n",
    "            elif part == 2:\n",
    "                length_1 = lengths_1[batch_idx]\n",
    "                length_2 = limits[batch_idx]\n",
    "            else:\n",
    "                length_1 = limits[batch_idx]\n",
    "                length_2 = lengths_2[batch_idx]\n",
    "            part_1 = tokens[:length_1]\n",
    "            if length_2 > 0:\n",
    "                part_2 = tokens[-length_2:]\n",
    "            else:\n",
    "                part_2 = []\n",
    "            if self.second_part_as_sequence_B:\n",
    "                sliced_batch_A.append(part_1)\n",
    "                sliced_batch_B.append(part_2)\n",
    "            else:\n",
    "                sliced_batch_A.append(part_1 + part_2)\n",
    "        if self.second_part_as_sequence_B:\n",
    "            tokenised = self.tokeniser(\n",
    "               sliced_batch_A, sliced_batch_B,\n",
    "               is_split_into_words = True,\n",
    "            )\n",
    "        else:\n",
    "            tokenised = self.tokeniser(\n",
    "               sliced_batch_A,\n",
    "               is_split_into_words = True,\n",
    "            )\n",
    "        # check lengths in subword pieced\n",
    "        retval = []\n",
    "        for batch_idx, subword_ids in enumerate(tokenised['input_ids']):\n",
    "            if part == 1:\n",
    "                length = len(subword_ids) - 2  # account for [CLS] and [SEP]\n",
    "                if self.second_part_as_sequence_B:\n",
    "                    length -= 1                # account for second [SEP] token\n",
    "                fit = length <= self.first_sequence_length\n",
    "            else:\n",
    "                fit = len(subword_ids) <= self.max_sequence_length\n",
    "            retval.append(fit)\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da14143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedDocuments(SlicedDocuments_part_5):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            assert isinstance(idx, int)\n",
    "        parts, label = self.slices[idx]\n",
    "        retval = {}\n",
    "        retval['parts'] = parts\n",
    "        retval['label'] = label\n",
    "        return retval\n",
    "    \n",
    "    def expand_lengths(self, batch, lengths_1, lengths_2):\n",
    "        ''' pushes lengths_1 as far out as possible '''\n",
    "        return self.get_lengths(\n",
    "            batch, part = 3,\n",
    "            lengths_1 = lengths_1,\n",
    "            lengths_2 = lengths_2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87294acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "splits = data_loader.get_xval_splits()\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529fe788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 training documents ready in x-val fold 0\n"
     ]
    }
   ],
   "source": [
    "# this may take a minute\n",
    "\n",
    "raw_data = splits[0][0]\n",
    "sliced_docs = SlicedDocuments(\n",
    "    raw_data, tokeniser,\n",
    "    fraction_for_first_sequence = 0.25,\n",
    "    max_sequence_length = max_sequence_length, # 99,\n",
    "    second_part_as_sequence_B = True,\n",
    ")\n",
    "print('%d training documents ready in x-val fold 0' %len(sliced_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc4b827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_idx seq_len tokens1 tokens2 total\n",
      "      0     256      59     175   234\n",
      "      1     256      59     159   218\n",
      "      2     256      50     167   217\n",
      "      3     256      53     178   231\n",
      "      4     256      46     177   223\n",
      "      5     256      50     150   200\n",
      "      6     256      61     148   209\n",
      "      7     256      58     171   229\n",
      "      8     256      51     174   225\n",
      "      9     256      48     146   194\n",
      "\n",
      "First and last 3 sentences of doc_idx 906 with 53 seq_len:\n",
      "[0] ['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.']\n",
      "Selected slice 1: ['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.']\n",
      "Selected slice 2: ['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.']\n",
      "\n",
      "Frequency of each sequence length:\n",
      "53 1\n",
      "243 1\n",
      "254 5\n",
      "255 35\n",
      "256 1758\n"
     ]
    }
   ],
   "source": [
    "length2count = defaultdict(lambda: 0)\n",
    "\n",
    "print('doc_idx seq_len tokens1 tokens2 total')\n",
    "for doc_idx, sliced_doc in enumerate(sliced_docs):\n",
    "    parts = sliced_doc['parts']\n",
    "    if len(parts) == 2:\n",
    "        tokenised = tokeniser(\n",
    "           [parts[0]],\n",
    "           [parts[1]],\n",
    "           is_split_into_words = True,\n",
    "        )\n",
    "        tokens1 = len(parts[0])\n",
    "        tokens2 = len(parts[1])\n",
    "    else:\n",
    "        tokenised = tokeniser(\n",
    "           [parts[0]],\n",
    "           is_split_into_words = True,\n",
    "        )\n",
    "        tokens1 = len(parts[0])\n",
    "        tokens2 = 0\n",
    "    length = len(tokenised['input_ids'][0])\n",
    "    length2count[length] += 1             \n",
    "\n",
    "    if length < 90:\n",
    "        print(\n",
    "            '\\nFirst and last 3 sentences of doc_idx', doc_idx,\n",
    "            'with', length, 'seq_len:'\n",
    "        )\n",
    "        sent_idx = set()\n",
    "        for idx in range(3):\n",
    "            sent_idx.add(idx)\n",
    "            idx = len(raw_data[doc_idx][0]) - 1 - idx\n",
    "            if idx >= 0:\n",
    "                sent_idx.add(idx)\n",
    "        for s_idx in sorted(list(sent_idx)):\n",
    "            try:\n",
    "                sentence = raw_data[doc_idx][0][s_idx]\n",
    "                print('[%d] %r' %(s_idx, sentence))\n",
    "            except IndexError:\n",
    "                pass\n",
    "        print('Selected slice 1:', parts[0])\n",
    "        if len(parts) == 2:\n",
    "            print('Selected slice 2:', parts[1])\n",
    "    if doc_idx < 10:\n",
    "        print('%7d %7d %7d %7d %5d' %(\n",
    "            doc_idx, length, tokens1, tokens2, tokens1+tokens2,\n",
    "        ))\n",
    "        \n",
    "print('\\nFrequency of each sequence length:')        \n",
    "for length in sorted(list(length2count.keys())):\n",
    "    print(length, length2count[length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69811e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "class SlicedDataModule_Part_1(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, classifier, data_split = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hparams = classifier.hparams\n",
    "        self.classifier = classifier\n",
    "        if data_split is None:\n",
    "            # this happens when loading a checkpoint\n",
    "            data_split = (None, None, None)\n",
    "        elif len(data_split) == 2:\n",
    "            # add empty validation set\n",
    "            tr_data, val_data = self.split(data_split[0], 0.9)\n",
    "            data_split = (tr_data, val_data, data_split[1])\n",
    "        self.data_split = data_split\n",
    "        self.kwargs = kwargs\n",
    "        self.label_encoder = LabelEncoder(\n",
    "            ['pos', 'neg'],\n",
    "            reserved_labels = [],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        assert self.hparams.batch_size <= batch_size\n",
    "        dataset = SlicedDocuments(\n",
    "            raw_data = self.data_split[0],\n",
    "            **self.kwargs\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset = dataset,\n",
    "            sampler     = RandomSampler(dataset),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "035caf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlicedDataModule(SlicedDataModule_Part_1):\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        assert self.hparams.batch_size <= batch_size\n",
    "        if not self.data_split[1]:\n",
    "            return None   # TODO: check documentation what to return\n",
    "        return DataLoader(\n",
    "            dataset = SlicedDocuments(\n",
    "                raw_data = self.data_split[1],\n",
    "                **self.kwargs\n",
    "            ),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        assert self.hparams.batch_size <= batch_size\n",
    "        return DataLoader(\n",
    "            dataset = SlicedDocuments(\n",
    "                raw_data = self.data_split[2],\n",
    "                **self.kwargs\n",
    "            ),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def split(self, data, ratio):\n",
    "        # get label distribution:\n",
    "        distribution = defaultdict(lambda: 0)\n",
    "        for _, label in data:\n",
    "            distribution[label] += 1\n",
    "        # get target frequencies of labels in first set\n",
    "        still_needed = defaultdict(lambda: 0)\n",
    "        for label in distribution:\n",
    "            still_needed[label] = int(ratio*distribution[label])\n",
    "        # split data accordingly\n",
    "        dataset_1 = []\n",
    "        dataset_2 = []\n",
    "        for item in data:\n",
    "            label = item[1]\n",
    "            if still_needed[label] > 0:\n",
    "                dataset_1.append(item)\n",
    "                still_needed[label] -= 1\n",
    "            else:\n",
    "                dataset_2.append(item)\n",
    "        return dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b0363ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_part_1(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams = None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if type(hparams) is dict:\n",
    "            #print('Converting', type(hparams))\n",
    "            hparams = pl.utilities.AttributeDict(hparams)\n",
    "        #print('New classifier with', hparams)\n",
    "        self.hparams = hparams\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.data = SlicedDataModule(self, **kwargs)\n",
    "        if 'tokeniser' in kwargs:\n",
    "            self.tokenizer = kwargs['tokeniser']  # attribute expected by lightning\n",
    "        else:\n",
    "            # this happens when loading a checkpoint\n",
    "            self.tokenizer = None  # TODO: this may break ability to use the model\n",
    "        self.__build_model()\n",
    "        self.__build_loss()\n",
    "        if hparams.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = hparams.nr_frozen_epochs\n",
    "        self.record_predictions = False\n",
    "            \n",
    "    def __build_model(self) -> None:\n",
    "        ''' Init BERT model, tokeniser and classification head '''\n",
    "        # Q: Why not use AutoModelForSequenceClassification?\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            model_name,  # was: self.hparams.encoder_model\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1536),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1536, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, self.data.label_encoder.vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __build_loss(self):\n",
    "        self._loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ecb8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_part_1(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hparams = None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if type(hparams) is dict:\n",
    "            #print('Converting', type(hparams))\n",
    "            hparams = pl.utilities.AttributeDict(hparams)\n",
    "        #print('New classifier with', hparams)\n",
    "        self.hparams = hparams\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.data = SlicedDataModule(self, **kwargs)\n",
    "        if 'tokeniser' in kwargs:\n",
    "            self.tokenizer = kwargs['tokeniser']  # attribute expected by lightning\n",
    "        else:\n",
    "            # this happens when loading a checkpoint\n",
    "            self.tokenizer = None  # TODO: this may break ability to use the model\n",
    "        self.__build_model()\n",
    "        self.__build_loss()\n",
    "        if hparams.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = hparams.nr_frozen_epochs\n",
    "        self.record_predictions = False\n",
    "            \n",
    "    def __build_model(self) -> None:\n",
    "        ''' Init BERT model, tokeniser and classification head '''\n",
    "        # Q: Why not use AutoModelForSequenceClassification?\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            model_name,  # was: self.hparams.encoder_model\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1536),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1536, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, self.data.label_encoder.vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __build_loss(self):\n",
    "        self._loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6c59629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "\n",
    "class Classifier_part_2(Classifier_part_1):\n",
    "    \n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        if self._frozen:\n",
    "            log.info('\\n== Encoder model fine-tuning ==')\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "            \n",
    "    def freeze_encoder(self) -> None:\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def predict(self, sample: dict) -> dict:\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_inputs, _ = self.prepare_sample(\n",
    "                [sample],\n",
    "                prepare_target = False\n",
    "            )\n",
    "            model_out = self.forward(batch_inputs)\n",
    "            logits = torch.Tensor.cpu(model_out[\"logits\"]).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits, axis=1)\n",
    "            ]\n",
    "            sample[\"predicted_label\"] = predicted_labels[0]\n",
    "        return sample\n",
    "    \n",
    "    def start_recording_predictions(self):\n",
    "        self.record_predictions = True\n",
    "        self.reset_recorded_predictions()\n",
    "        \n",
    "    def stop_recording_predictions(self):\n",
    "        self.record_predictions = False\n",
    "        \n",
    "    def reset_recorded_predictions(self):\n",
    "        self.seq2label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aed842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.utils import lengths_to_mask\n",
    "\n",
    "class Classifier_part_3(Classifier_part_2):\n",
    "    \n",
    "    def forward(self, batch_input):\n",
    "        tokens  = batch_input['input_ids']\n",
    "        lengths = batch_input['length']\n",
    "        mask = batch_input['attention_mask']\n",
    "        # Run BERT model.\n",
    "        word_embeddings = self.bert(tokens, mask).last_hidden_state\n",
    "        sentemb = word_embeddings[:,0]  # at position of [CLS]\n",
    "        logits = self.classification_head(sentemb)\n",
    "        # Hack to conveniently use the model and trainer to\n",
    "        # get predictions for a test set:\n",
    "        if self.record_predictions:\n",
    "            logits_np = torch.Tensor.cpu(logits).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits_np, axis=1)\n",
    "            ]\n",
    "            for index, input_token_ids in enumerate(tokens):\n",
    "                key = torch.Tensor.cpu(input_token_ids).numpy().tolist()\n",
    "                # truncate trailing zeros\n",
    "                while key and key[-1] == 0:\n",
    "                    del key[-1]\n",
    "                self.seq2label[tuple(key)] = predicted_labels[index]\n",
    "        return {\"logits\": logits}\n",
    "    \n",
    "    def loss(self, predictions: dict, targets: dict) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes Loss value according to a loss function.\n",
    "        :param predictions: model specific output. Must contain a key 'logits' with\n",
    "            a tensor [batch_size x 1] with model predictions\n",
    "        :param labels: Label values [batch_size]\n",
    "        Returns:\n",
    "            torch.tensor with loss value.\n",
    "        \"\"\"\n",
    "        return self._loss(predictions[\"logits\"], targets[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b619b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_part_4(Classifier_part_3):\n",
    "    \n",
    "    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n",
    "        \"\"\"\n",
    "        Function that prepares a sample to input the model.\n",
    "        :param sample: list of dictionaries.\n",
    "        \n",
    "        Returns:\n",
    "            - dictionary with the expected model inputs.\n",
    "            - dictionary with the expected target labels.\n",
    "        \"\"\"\n",
    "        assert len(sample) <= batch_size\n",
    "        assert self.tokenizer is not None\n",
    "        with_1_part = 0\n",
    "        with_2_parts = 0\n",
    "        batch_part_1 = []\n",
    "        batch_part_2 = []\n",
    "        for item in sample:\n",
    "            parts = item['parts']\n",
    "            if len(parts) == 2:\n",
    "                with_2_parts += 1\n",
    "                batch_part_1.append(parts[0])\n",
    "                batch_part_2.append(parts[1])\n",
    "            else:\n",
    "                with_1_part += 1\n",
    "                batch_part_1.append(parts[0])\n",
    "        assert not (with_1_part and with_2_parts)\n",
    "        kwargs = {\n",
    "            'is_split_into_words': True,\n",
    "            'return_length':       True,\n",
    "            'padding':             'max_length',\n",
    "            # https://github.com/huggingface/transformers/issues/8691\n",
    "            'return_tensors':      'pt',\n",
    "        }\n",
    "        if with_2_parts:\n",
    "            encoded_batch = self.tokenizer(\n",
    "                batch_part_1,\n",
    "                batch_part_2,\n",
    "                **kwargs\n",
    "            )\n",
    "        else:\n",
    "            encoded_batch = self.tokenizer(\n",
    "                batch_part_1,\n",
    "                **kwargs\n",
    "            )\n",
    "        if not prepare_target:\n",
    "            return encoded_batch, {}\n",
    "        # Prepare target:\n",
    "        batch_labels = []\n",
    "        for item in sample:\n",
    "            batch_labels.append(item['label'])\n",
    "        assert len(batch_labels) <= batch_size\n",
    "        try:\n",
    "            targets = {\n",
    "                \"labels\": self.data.label_encoder.batch_encode(batch_labels)\n",
    "            }\n",
    "            return encoded_batch, targets\n",
    "        except RuntimeError:\n",
    "            raise Exception(\"Label encoder found an unknown label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdc8f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Classifier_part_5(Classifier_part_4):\n",
    "    \n",
    "    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        # Q: What is this about?\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "        output = OrderedDict({\"loss\": loss_val})\n",
    "        self.log('train_loss', loss_val, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "   \n",
    "    def test_or_validation_step(self, test_type, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        y = targets[\"labels\"]\n",
    "        y_hat = model_out[\"logits\"]\n",
    "        # acc\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "        output = OrderedDict({\n",
    "            test_type + \"_loss\": loss_val,\n",
    "            test_type + \"_acc\":  val_acc,\n",
    "            'batch_size': len(batch),\n",
    "            #'predictions': labels_hat,\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'val', batch, batch_nb, *args, **kwargs\n",
    "        )\n",
    "    \n",
    "    def test_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'test', batch, batch_nb, *args, **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed496b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class Classifier(Classifier_part_5):\n",
    "    \n",
    "    # validation_end() is now validation_epoch_end()\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/blob/efd272a3cac2c412dd4a7aa138feafb2c114326f/CHANGELOG.md\n",
    "    \n",
    "    def test_or_validation_epoch_end(self, test_type, outputs: list) -> None:\n",
    "        val_loss_mean = 0.0\n",
    "        val_acc_mean = 0.0\n",
    "        total_size = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output[test_type + \"_loss\"]\n",
    "            # reduce manually when using dp\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output[test_type + \"_acc\"]\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_acc = torch.mean(val_acc)\n",
    "            # We weight the batch accuracy by batch size to not give\n",
    "            # higher weight to the items of a smaller, final bacth.\n",
    "            batch_size = output['batch_size']\n",
    "            val_acc_mean += val_acc * batch_size\n",
    "            total_size += batch_size\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= total_size\n",
    "        self.log(test_type+'_loss', val_loss_mean)\n",
    "        self.log(test_type+'_acc',  val_acc_mean)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('val', outputs)\n",
    "                                     \n",
    "    def test_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('test', outputs)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n",
    "        parameters = [\n",
    "            {\"params\": self.classification_head.parameters()},\n",
    "            {\n",
    "                \"params\": self.bert.parameters(),\n",
    "                \"lr\": self.hparams.encoder_learning_rate,\n",
    "                #\"weight_decay\": 0.01,  # TODO: try this as it is in the BERT paper\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.Adam(parameters, lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            self.unfreeze_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "981e958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92faa24ea33d4ade8ce620024a17a88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "xval_run = 0  # 0 to 9\n",
    "\n",
    "\n",
    "#class DotDict(pl.utilities.AttributeDict):    \n",
    "#    __getattr__ = dict.get\n",
    "\n",
    "    # the above misses pickle support; presumable dict defines custom pickle behaviour\n",
    "    # that causes the unpickled object to be a plain dict object\n",
    "\n",
    "print('batch_size', batch_size)\n",
    "\n",
    "classifier = Classifier(\n",
    "    hparams = {\n",
    "        \"encoder_learning_rate\": 1e-05,  # Encoder specific learning rate\n",
    "        \"learning_rate\":         3e-05,  # Classification head learning rate\n",
    "        \"nr_frozen_epochs\":      3,      # Number of epochs we want to keep the encoder model frozen\n",
    "        \"loader_workers\":        4,      # How many subprocesses to use for data loading.\n",
    "                                         # (0 means that the data will be loaded in the main process)\n",
    "        \"batch_size\":            batch_size,\n",
    "        \"gpus\":                  1,\n",
    "    },\n",
    "    # parameters for SlicedDataModule:\n",
    "    data_split = splits[xval_run],\n",
    "    # parameters for SlicedDocument():\n",
    "    tokeniser                   = tokeniser,\n",
    "    fraction_for_first_sequence = 0.0,   # set to 0.0001 to duplicate short documents\n",
    "    max_sequence_length         = max_sequence_length,\n",
    "    second_part_as_sequence_B   = False,\n",
    "    preproc_batch_size          = 8\n",
    ")   \n",
    "print('Ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c7877dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "You requested GPUs: [0]\n But your machine only has: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e70047ba578f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcheck_val_every_n_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloggers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorBoardLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lightning_logs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BERT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\env_vars_connector.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# all args were already moved to kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BERT\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logger, checkpoint_callback, callbacks, default_root_dir, gradient_clip_val, process_position, num_nodes, num_processes, gpus, auto_select_gpus, tpu_cores, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, sync_batchnorm, precision, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, distributed_backend, automatic_optimization, move_metrics_to_cpu, enable_pl_optimizer, multiple_trainloader_mode, stochastic_weight_avg)\u001b[0m\n\u001b[0;32m    319\u001b[0m         self.accelerator_connector = AcceleratorConnector(\n\u001b[0;32m    320\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpu_cores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistributed_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto_select_gpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msync_batchnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mreplace_sampler_ddp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamp_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamp_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         )\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger_connector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoggerConnector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_gpu_memory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BERT\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_processes, tpu_cores, distributed_backend, auto_select_gpus, gpus, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpick_multiple_gpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_device_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_gpu_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_distributed_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BERT\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py\u001b[0m in \u001b[0;36mparse_gpu_ids\u001b[1;34m(gpus)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GPUs requested but none are available.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mgpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sanitize_gpu_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BERT\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py\u001b[0m in \u001b[0;36m_sanitize_gpu_ids\u001b[1;34m(gpus)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgpu\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_available_gpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             raise MisconfigurationException(\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[1;34mf\"You requested GPUs: {gpus}\\n But your machine only has: {all_available_gpus}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             )\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: You requested GPUs: [0]\n But your machine only has: []"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor   = 'val_acc',\n",
    "    min_delta = 0.00,\n",
    "    patience  = 5,\n",
    "    verbose   = False,\n",
    "    mode      = 'max',\n",
    ")\n",
    "\n",
    "save_top_model_callback = ModelCheckpoint(\n",
    "    save_top_k = 3,\n",
    "    monitor    = 'val_acc',\n",
    "    mode       = 'max',\n",
    "    filename   = '{val_acc:.4f}-{epoch:02d}-{val_loss:.4f}'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stop_callback, save_top_model_callback],\n",
    "    max_epochs = 6,\n",
    "    min_epochs = classifier.hparams.nr_frozen_epochs + 2,\n",
    "    gpus = classifier.hparams.gpus,\n",
    "    accumulate_grad_batches = 4,   # compensate for small batch size\n",
    "    #limit_train_batches = 10,  # use only a subset of the data during development for higher speed\n",
    "    check_val_every_n_epoch = 1,\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "start = time.time()\n",
    "trainer.fit(classifier, classifier.data)\n",
    "print('Training time: %.0f minutes' %((time.time()-start)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51e4dda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41ca72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
