{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing I needed to do was replicate the baseline Naive Bayes which was provided to us as part of the assignment and evaluate its performance so I would have something to compare my further experiments to. The next 10 cells are not much different to what was supplied and are simply setting up the data loading etc.\n",
    "\n",
    "If you skip to the \"Naive Bayes\" sub heading, this would be a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "class PL04DataLoader_Part_1:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_labelled_dataset(self, fold = 0):\n",
    "        ''' Compile a fold of the data set\n",
    "        '''\n",
    "        dataset = []\n",
    "        for label in ('pos', 'neg'):\n",
    "            for document in self.get_documents(\n",
    "                fold = fold,\n",
    "                label = label,\n",
    "            ):\n",
    "                dataset.append((document, label))\n",
    "        return dataset\n",
    "    \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        ''' Enumerate the raw contents of all data set files.\n",
    "            Args:\n",
    "                data_dir: relative or absolute path to the data set folder\n",
    "                fold: which fold to load (0 to n_folds-1)\n",
    "                label: 'pos' or 'neg' to\n",
    "                    select data with positive or negative sentiment\n",
    "                    polarity\n",
    "            Return:\n",
    "                List of tokenised documents, each a list of sentences\n",
    "                that in turn are lists of tokens\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "class PL04DataLoader(PL04DataLoader_Part_1):\n",
    "    \n",
    "    def get_xval_splits(self):\n",
    "        ''' Split data with labels for cross-validation\n",
    "            returns a list of k pairs (training_data, test_data)\n",
    "            for k cross-validation\n",
    "        '''\n",
    "        # load the folds\n",
    "        folds = []\n",
    "        for i in range(10):\n",
    "            folds.append(self.get_labelled_dataset(\n",
    "                fold = i\n",
    "            ))\n",
    "        # create training-test splits\n",
    "        retval = []\n",
    "        for i in range(10):\n",
    "            test_data = folds[i]\n",
    "            training_data = []\n",
    "            for j in range(9):\n",
    "                ij1 = (i+j+1) % 10\n",
    "                assert ij1 != i\n",
    "                training_data = training_data + folds[ij1]\n",
    "            retval.append((training_data, test_data))\n",
    "        return retval\n",
    "    \n",
    "class PL04DataLoaderFromStream(PL04DataLoader):\n",
    "        \n",
    "    def __init__(self, tgz_stream, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = {}\n",
    "        counter = 0\n",
    "        with tarfile.open(\n",
    "            mode = 'r|gz',\n",
    "            fileobj = tgz_stream\n",
    "        ) as tar_archive:\n",
    "            for tar_member in tar_archive:\n",
    "                if counter == 2000:\n",
    "                    break\n",
    "                path_components = tar_member.name.split('/')\n",
    "                filename = path_components[-1]\n",
    "                if filename.startswith('cv') \\\n",
    "                and filename.endswith('.txt') \\\n",
    "                and '_' in filename:\n",
    "                    label = path_components[-2]\n",
    "                    fold = int(filename[2])\n",
    "                    key = (fold, label)\n",
    "                    if key not in self.data:\n",
    "                        self.data[key] = []\n",
    "                    f = tar_archive.extractfile(tar_member)\n",
    "                    document = [\n",
    "                        line.decode('utf-8').split()\n",
    "                        for line in f.readlines()\n",
    "                    ]\n",
    "                    self.data[key].append(document)\n",
    "                    counter += 1\n",
    "            \n",
    "    def get_documents(self, fold = 0, label = 'pos'):\n",
    "        return self.data[(fold, label)]\n",
    "\n",
    "class PL04DataLoaderFromTGZ(PL04DataLoaderFromStream):\n",
    "    \n",
    "    def __init__(self, data_path, **kwargs):\n",
    "        with open(data_path, 'rb') as tgz_stream:\n",
    "            super().__init__(tgz_stream, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_entries = os.listdir()\n",
    "dir_entries.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = PL04DataLoaderFromTGZ('data.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== pos ==\n",
      "doc sentences start of first sentence\n",
      "  0      25   films|adapted|from|comic|books|have|had|plenty|of|success|,|whether\n",
      "  1      39   every|now|and|then|a|movie|comes|along|from|a|suspect|studio|,|with\n",
      "  2      19   you've|got|mail|works|alot|better|than|it|deserves|to|.|in|order|to|make\n",
      "  3      42   \"|jaws|\"|is|a|rare|film|that|grabs|your|attention|before|it|shows|you|a\n",
      "  4      25   moviemaking|is|a|lot|like|being|the|general|manager|of|an|nfl|team|in\n",
      "== neg ==\n",
      "doc sentences start of first sentence\n",
      "  0      35   plot|:|two|teen|couples|go|to|a|church|party|,|drink|and|then|drive|.\n",
      "  1      13   the|happy|bastard's|quick|movie|review|damn|that|y2k|bug|.|it's|got|a\n",
      "  2      23   it|is|movies|like|these|that|make|a|jaded|movie|viewer|thankful|for|the\n",
      "  3      19   \"|quest|for|camelot|\"|is|warner|bros|.|'|first|feature-length|,\n",
      "  4      37   synopsis|:|a|mentally|unstable|man|undergoing|psychotherapy|saves|a|boy\n"
     ]
    }
   ],
   "source": [
    "# test \"get_documents()\"\n",
    "\n",
    "def get_document_preview(document, max_length = 72):\n",
    "    s = []\n",
    "    count = 0\n",
    "    reached_limit = False\n",
    "    for sentence in document:\n",
    "        for token in sentence:\n",
    "            if count + len(token) + len(s) > max_length:\n",
    "                reached_limit = True\n",
    "                break\n",
    "            s.append(token)\n",
    "            count += len(token)\n",
    "        if reached_limit:\n",
    "            break\n",
    "    return '|'.join(s)\n",
    "    \n",
    "for label in 'pos neg'.split():\n",
    "    print(f'== {label} ==')\n",
    "    print('doc sentences start of first sentence')\n",
    "    for index, document in enumerate(data_loader.get_documents(\n",
    "        label = label\n",
    "    )):\n",
    "        print('%3d %7d   %s' %(\n",
    "            index, len(document), get_document_preview(document)\n",
    "        ))\n",
    "        if index == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr-size te-size (number of documents)\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n",
      "   1800     200\n"
     ]
    }
   ],
   "source": [
    "# test \"get_xval_splits()\"\n",
    "\n",
    "splits = data_loader.get_xval_splits()\n",
    "\n",
    "print('tr-size te-size (number of documents)')\n",
    "for xval_tr_data, xval_te_data in splits:\n",
    "    print('%7d %7d' %(len(xval_tr_data), len(xval_te_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorInterface:\n",
    "\n",
    "    def train(self, data_with_labels):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, data):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorWithVocabulary(PolarityPredictorInterface):\n",
    "    \n",
    "    def train(self, data_with_labels):\n",
    "        self.reset_vocab()\n",
    "        self.add_to_vocab_from_data(data_with_labels)\n",
    "        self.finalise_vocab()\n",
    "        tr_features = self.extract_features(\n",
    "            data_with_labels\n",
    "        )\n",
    "        tr_targets = self.get_targets(data_with_labels)\n",
    "        self.train_model_on_features(tr_features, tr_targets)\n",
    "        \n",
    "    def reset_vocab(self):\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def add_to_vocab_from_data(self, data):\n",
    "        for document, label in data:\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    self.vocab.add(token)\n",
    "\n",
    "    def finalise_vocab(self):\n",
    "        self.vocab = list(self.vocab)\n",
    "        # create reverse map for fast token lookup\n",
    "        self.token2index = {}\n",
    "        for index, token in enumerate(self.vocab):\n",
    "            self.token2index[token] = index\n",
    "        \n",
    "    def extract_features(self, data):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_targets(self, data, label2index = None):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class PolarityPredictorWithBagOfWords_01(PolarityPredictorWithVocabulary):\n",
    "    \n",
    "    def __init__(self, clip_counts = True):\n",
    "        self.clip_counts = clip_counts\n",
    "        \n",
    "    def extract_features(self, data):\n",
    "        # create numpy array of required size\n",
    "        columns = len(self.vocab)\n",
    "        rows = len(data)\n",
    "        features = numpy.zeros((rows, columns), dtype=numpy.int32)        \n",
    "        # populate feature matrix\n",
    "        for row, item in enumerate(data):\n",
    "            document, _ = item\n",
    "            for sentence in document:\n",
    "                for token in sentence:\n",
    "                    try:\n",
    "                        index = self.token2index[token]\n",
    "                    except KeyError:\n",
    "                        # token not in vocab\n",
    "                        # --> skip this token\n",
    "                        # --> continue with next token\n",
    "                        continue\n",
    "                    if self.clip_counts:\n",
    "                        features[row, index] = 1\n",
    "                    else:\n",
    "                        features[row, index] += 1\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolarityPredictorWithBagOfWords(PolarityPredictorWithBagOfWords_01):\n",
    " \n",
    "    def get_targets(self, data):\n",
    "        ''' create column vector with target labels\n",
    "        '''\n",
    "        # prepare target vector\n",
    "        targets = numpy.zeros(len(data), dtype=numpy.int8)\n",
    "        index = 0\n",
    "        for _, label in data:\n",
    "            if label == 'pos':\n",
    "                targets[index] = 1\n",
    "            index += 1\n",
    "        return targets\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "These next few cells set up the baseline Naive Bayes model that we were supplied with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "class PolarityPredictorBowNB(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train NB\n",
    "        self.model = MultinomialNB()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first functionality test\n",
    "\n",
    "model = PolarityPredictorBowNB()\n",
    "model.train(splits[0][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 pos neg films|adapted|from|comic|books|have|had|plenty|of|success|,|whether\n",
      "   1 pos pos every|now|and|then|a|movie|comes|along|from|a|suspect|studio|,|with\n",
      "   2 pos pos you've|got|mail|works|alot|better|than|it|deserves|to|.|in|order|to|make\n",
      "   3 pos pos \"|jaws|\"|is|a|rare|film|that|grabs|your|attention|before|it|shows|you|a\n",
      "   4 pos neg moviemaking|is|a|lot|like|being|the|general|manager|of|an|nfl|team|in\n",
      "   5 pos pos on|june|30|,|1960|,|a|self-taught|,|idealistic|,|yet|pragmatic|,|young\n",
      "   6 pos pos apparently|,|director|tony|kaye|had|a|major|battle|with|new|line\n",
      "   7 pos pos one|of|my|colleagues|was|surprised|when|i|told|her|i|was|willing|to|see\n",
      "   8 pos pos after|bloody|clashes|and|independence|won|,|lumumba|refused|to|pander|to\n",
      "   9 pos pos the|american|action|film|has|been|slowly|drowning|to|death|in|a|sea|of\n",
      "  10 pos pos after|watching|\"|rat|race|\"|last|week|,|i|noticed|my|cheeks|were|sore\n",
      "  11 pos pos i've|noticed|something|lately|that|i've|never|thought|of|before|.\n"
     ]
    }
   ],
   "source": [
    "def print_first_predictions(model, te_data, n = 12):\n",
    "    predictions = model.predict(te_data)\n",
    "    for i in range(n):\n",
    "        document, label = te_data[i]\n",
    "        prediction = predictions[i]\n",
    "        print('%4d %s %s %s' %(\n",
    "            i, label, prediction,\n",
    "            get_document_preview(document),\n",
    "        ))\n",
    "    \n",
    "print_first_predictions(model, splits[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985\n",
      "[[100   0]\n",
      " [  3  97]]\n"
     ]
    }
   ],
   "source": [
    "labels, accuracy, confusion_matrix = model.predict(\n",
    "    splits[0][1], get_accuracy = True, get_confusion_matrix = True\n",
    ")\n",
    "\n",
    "print(accuracy)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I decided that the \"accuracy\" mettric that is used in the provided notebook should be improved upon as it can be a misleading value. Precision, Recall and F1 score are better for binary classification tasks as they allow us to account for imbalances in our datasets.\n",
    "\n",
    "The cell below contains the function that is used to evaluate model performance and I have edited it from the original so it now reports these additional metrics and its final output is F1 score rather than accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.795\n",
      "Precision --> 0.82\n",
      "Recall --> 0.780952380952381\n",
      "F1 --> 0.8\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.84\n",
      "Precision --> 0.89\n",
      "Recall --> 0.8090909090909091\n",
      "F1 --> 0.8476190476190476\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.84\n",
      "Precision --> 0.86\n",
      "Recall --> 0.8269230769230769\n",
      "F1 --> 0.8431372549019608\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.825\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8155339805825242\n",
      "F1 --> 0.8275862068965517\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.835\n",
      "Precision --> 0.85\n",
      "Recall --> 0.8252427184466019\n",
      "F1 --> 0.8374384236453202\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.83\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8235294117647058\n",
      "F1 --> 0.8316831683168315\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.84\n",
      "Precision --> 0.87\n",
      "Recall --> 0.8207547169811321\n",
      "F1 --> 0.8446601941747572\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.89\n",
      "Recall --> 0.8165137614678899\n",
      "F1 --> 0.8516746411483254\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.785\n",
      "Precision --> 0.83\n",
      "Recall --> 0.7614678899082569\n",
      "F1 --> 0.7942583732057417\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.855\n",
      "Precision --> 0.92\n",
      "Recall --> 0.8141592920353983\n",
      "F1 --> 0.863849765258216\n",
      "\n",
      "(0.8341907075166752, 0.02170123140569834, 0.7942583732057417, 0.863849765258216)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, splits, verbose = False):\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    fold = 0\n",
    "    for tr_data, te_data in splits:\n",
    "        if verbose:\n",
    "            print('Evaluating fold %d of %d' %(fold+1, len(splits)))\n",
    "            fold += 1\n",
    "        model.train(tr_data)\n",
    "        _, accuracy, confusion_matrix = model.predict(te_data, get_accuracy = True, get_confusion_matrix = True)\n",
    "        \n",
    "        tp, fp, fn, tn = confusion_matrix[0][0], confusion_matrix[0][1], confusion_matrix[1][0], confusion_matrix[1][1]\n",
    "        prec = tp/(tp + fp)\n",
    "        rec = tp/(tp + fn)\n",
    "        f1 = (2*prec*rec)/(prec+rec)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "        if verbose:\n",
    "            print('Accuracy -->', accuracy)\n",
    "            print('Precision -->', prec)\n",
    "            print('Recall -->', rec)\n",
    "            print('F1 -->', f1)\n",
    "            print()\n",
    "    n = float(len(accuracies))\n",
    "    avg = sum(f1s) / n\n",
    "    mse = sum([(x-avg)**2 for x in accuracies]) / n\n",
    "    return (avg, mse**0.5, min(f1s),\n",
    "            max(f1s))\n",
    "\n",
    "# this takes about 3 minutes\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score after 10 fold cross validation turned out to be **0.834**\n",
    "\n",
    "The first step to try improve on this baseline was to use the same unchanged dataset with Logistic Regression instead of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "I imported ScikitLearn's Logistic Regression package for this step. First attempts to use this resulted in the model not converging for some of the 10 folds. \n",
    "\n",
    "This was due to the default iteration number being 100. Once I changed this to 1000 all 10 cross validation folds converged and I was able to get a true average F1 score for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "class PolarityPredictorBowLR(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8484848484848485\n",
      "F1 --> 0.8442211055276383\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.875\n",
      "Precision --> 0.9\n",
      "Recall --> 0.8571428571428571\n",
      "F1 --> 0.8780487804878048\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.87\n",
      "Recall --> 0.8285714285714286\n",
      "F1 --> 0.848780487804878\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.86\n",
      "Precision --> 0.84\n",
      "Recall --> 0.875\n",
      "F1 --> 0.8571428571428572\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.855\n",
      "Precision --> 0.84\n",
      "Recall --> 0.865979381443299\n",
      "F1 --> 0.8527918781725888\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.875\n",
      "Precision --> 0.91\n",
      "Recall --> 0.8504672897196262\n",
      "F1 --> 0.8792270531400966\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.865\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8842105263157894\n",
      "F1 --> 0.8615384615384616\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.885\n",
      "Precision --> 0.88\n",
      "Recall --> 0.8888888888888888\n",
      "F1 --> 0.8844221105527638\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.84\n",
      "Precision --> 0.89\n",
      "Recall --> 0.8090909090909091\n",
      "F1 --> 0.8476190476190476\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.915\n",
      "Precision --> 0.94\n",
      "Recall --> 0.8952380952380953\n",
      "F1 --> 0.9170731707317075\n",
      "\n",
      "(0.8670864952717844, 0.021568042840638336, 0.8442211055276383, 0.9170731707317075)\n"
     ]
    }
   ],
   "source": [
    "model = PolarityPredictorBowLR()\n",
    "\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score for Baseline Logistic Regression turned out to be **0.867** which was a nice increase on our Baseline Naive Bayes. The runtime was quite similar to the Naive Bayes too so there didn't seem to be any downsides.\n",
    "\n",
    "The next implementation that I wanted to try out was using ScikitLearn's Decision Tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class PolarityPredictorBowDT(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = DecisionTreeClassifier()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.585\n",
      "Precision --> 0.56\n",
      "Recall --> 0.5894736842105263\n",
      "F1 --> 0.5743589743589743\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.615\n",
      "Precision --> 0.6\n",
      "Recall --> 0.6185567010309279\n",
      "F1 --> 0.6091370558375634\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.655\n",
      "Precision --> 0.72\n",
      "Recall --> 0.6371681415929203\n",
      "F1 --> 0.6760563380281691\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.595\n",
      "Precision --> 0.59\n",
      "Recall --> 0.5959595959595959\n",
      "F1 --> 0.5929648241206029\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.62\n",
      "Precision --> 0.64\n",
      "Recall --> 0.6153846153846154\n",
      "F1 --> 0.6274509803921569\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.53\n",
      "Precision --> 0.47\n",
      "Recall --> 0.5340909090909091\n",
      "F1 --> 0.5\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.675\n",
      "Precision --> 0.69\n",
      "Recall --> 0.6699029126213593\n",
      "F1 --> 0.6798029556650246\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.625\n",
      "Precision --> 0.61\n",
      "Recall --> 0.6288659793814433\n",
      "F1 --> 0.6192893401015229\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.55\n",
      "Precision --> 0.56\n",
      "Recall --> 0.5490196078431373\n",
      "F1 --> 0.5544554455445545\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.685\n",
      "Precision --> 0.64\n",
      "Recall --> 0.7032967032967034\n",
      "F1 --> 0.6701570680628273\n",
      "\n",
      "(0.6103672982111396, 0.04801108018466082, 0.5, 0.6798029556650246)\n"
     ]
    }
   ],
   "source": [
    "model = PolarityPredictorBowDT()\n",
    "\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average F1 score for this was **0.610** which was quite a drop from the previous two.\n",
    "\n",
    "It also took noticeably longer time to complete the 10 folds so it seemed clear that this was definitely an inferior option.\n",
    "\n",
    "Next up was Support Vector Machine implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "class PolarityPredictorBowSVM(PolarityPredictorWithBagOfWords):\n",
    "\n",
    "    def train_model_on_features(self, tr_features, tr_targets):\n",
    "        # pass numpy array to sklearn to train Logistic Regression\n",
    "        # iterations set to 1000 as default of 100 didn't guarantee convergence with our data\n",
    "        self.model = svm.SVC()\n",
    "        self.model.fit(tr_features, tr_targets)\n",
    "        \n",
    "    def predict(\n",
    "        self, data, get_accuracy = False,\n",
    "        get_confusion_matrix = False\n",
    "    ):\n",
    "        features = self.extract_features(data)\n",
    "        # use numpy to get predictions\n",
    "        y_pred = self.model.predict(features)\n",
    "        # restore labels\n",
    "        labels = []\n",
    "        for is_positive in y_pred:\n",
    "            if is_positive:\n",
    "                labels.append('pos')\n",
    "            else:\n",
    "                labels.append('neg')\n",
    "        if get_accuracy or get_confusion_matrix:\n",
    "            retval = []\n",
    "            retval.append(labels)\n",
    "            y_true = self.get_targets(data)\n",
    "            if get_accuracy:\n",
    "                retval.append(\n",
    "                    metrics.accuracy_score(y_true, y_pred)\n",
    "                )\n",
    "            if get_confusion_matrix:\n",
    "                retval.append(\n",
    "                    metrics.confusion_matrix(y_true, y_pred)\n",
    "                )\n",
    "            return retval\n",
    "        else:\n",
    "            return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold 1 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.9\n",
      "Recall --> 0.8181818181818182\n",
      "F1 --> 0.8571428571428572\n",
      "\n",
      "Evaluating fold 2 of 10\n",
      "Accuracy --> 0.85\n",
      "Precision --> 0.91\n",
      "Recall --> 0.8125\n",
      "F1 --> 0.8584905660377358\n",
      "\n",
      "Evaluating fold 3 of 10\n",
      "Accuracy --> 0.825\n",
      "Precision --> 0.86\n",
      "Recall --> 0.8037383177570093\n",
      "F1 --> 0.8309178743961353\n",
      "\n",
      "Evaluating fold 4 of 10\n",
      "Accuracy --> 0.855\n",
      "Precision --> 0.84\n",
      "Recall --> 0.865979381443299\n",
      "F1 --> 0.8527918781725888\n",
      "\n",
      "Evaluating fold 5 of 10\n",
      "Accuracy --> 0.845\n",
      "Precision --> 0.83\n",
      "Recall --> 0.8556701030927835\n",
      "F1 --> 0.8426395939086294\n",
      "\n",
      "Evaluating fold 6 of 10\n",
      "Accuracy --> 0.865\n",
      "Precision --> 0.89\n",
      "Recall --> 0.8476190476190476\n",
      "F1 --> 0.8682926829268293\n",
      "\n",
      "Evaluating fold 7 of 10\n",
      "Accuracy --> 0.895\n",
      "Precision --> 0.88\n",
      "Recall --> 0.9072164948453608\n",
      "F1 --> 0.8934010152284264\n",
      "\n",
      "Evaluating fold 8 of 10\n",
      "Accuracy --> 0.885\n",
      "Precision --> 0.9\n",
      "Recall --> 0.8737864077669902\n",
      "F1 --> 0.8866995073891626\n",
      "\n",
      "Evaluating fold 9 of 10\n",
      "Accuracy --> 0.83\n",
      "Precision --> 0.84\n",
      "Recall --> 0.8235294117647058\n",
      "F1 --> 0.8316831683168315\n",
      "\n",
      "Evaluating fold 10 of 10\n",
      "Accuracy --> 0.9\n",
      "Precision --> 0.93\n",
      "Recall --> 0.8773584905660378\n",
      "F1 --> 0.9029126213592233\n",
      "\n",
      "(0.8624971764878421, 0.02472318528044945, 0.8309178743961353, 0.9029126213592233)\n"
     ]
    }
   ],
   "source": [
    "model = PolarityPredictorBowSVM()\n",
    "\n",
    "print(evaluate_model(model, splits, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was more like it. The average F1 score for Baseline SVM was **0.862**. It should be pointed out, however, that the SVM implementation took far longer than the previous ones.\n",
    "\n",
    "The overall takeaway at this point was that Decision Trees were not a good choice but that Naive Bayes, Logistic Regression and Support Vector Machine were all viable options with Logistic Regression leading the way.\n",
    "\n",
    "The next section details my attempt to add Negation Handling into the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
